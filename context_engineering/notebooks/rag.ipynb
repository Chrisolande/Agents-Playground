{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import END, MessagesState, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from utils import format_messages\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2025-05-01-thinking/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = [item for sublist in docs for item in sublist]\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=50\n",
    ")\n",
    "\n",
    "# Splits documents into chunks\n",
    "doc_splits = text_splitter.split_documents(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"local_files_only\": True},\n",
    ")\n",
    "vector_store = InMemoryVectorStore.from_documents(\n",
    "    documents=doc_splits, embedding=embeddings\n",
    ")\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='54a2026d-1154-4a8d-82bd-b344e1c5ce82', metadata={'source': 'https://lilianweng.github.io/posts/2025-05-01-thinking/', 'title': \"Why We Think | Lil'Log\", 'description': 'Special thanks to John Schulman for a lot of super valuable feedback and direct edits on this post.\\nTest time compute (Graves et al. 2016, Ling, et al. 2017, Cobbe et al. 2021) and Chain-of-thought (CoT) (Wei et al. 2022, Nye et al. 2021), have led to significant improvements in model performance, while raising many research questions. This post aims to review recent developments in how to effectively use test-time compute (i.e. “thinking time”) and why it helps.', 'language': 'en'}, page_content='Why We Think | Lil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Why We Think\\n    \\nDate: May 1, 2025  |  Estimated Reading Time: 40 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nMotivation\\n\\nAnalogy to Psychology\\n\\nComputation as a Resource\\n\\nLatent Variable Modeling\\n\\n\\nThinking in Tokens\\n\\nBranching and Editing\\n\\nParallel Sampling\\n\\nSequential Revision\\n\\n\\nRL for Better Reasoning\\n\\nExternal Tool Use\\n\\nThinking Faithfully\\n\\nDoes the Model Tell What it Thinks Faithfully\\n\\nOptimization Pressure on CoT: Good or Bad?\\n\\n\\n\\nThinking in Continuous Space\\n\\nRecurrent Architecture\\n\\nThinking Tokens\\n\\n\\nThinking as Latent Variables\\n\\nExpectation-Maximization\\n\\nIterative Learning\\n\\n\\nScaling Laws for Thinking Time\\n\\nWhat’s for Future\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nSpecial thanks to John Schulman for a lot of super valuable feedback and direct edits on this post.\\nTest time compute (Graves et al. 2016, Ling, et al. 2017, Cobbe et al. 2021) and Chain-of-thought (CoT) (Wei et al. 2022, Nye et al. 2021), have led to significant improvements in model performance, while raising many research questions. This post aims to review recent developments in how to effectively use test-time compute (i.e. “thinking time”) and why it helps.\\nMotivation#\\nEnabling models to think for longer can be motivated in a few different ways.\\nAnalogy to Psychology#\\nThe core idea is deeply connected to how humans think. We humans cannot immediately provide the answer for \"What\\'s 12345 times 56789?\". Rather, it is natural to spend time pondering and analyzing before getting to the result, especially for complex problems. In Thinking, Fast and Slow (Kahneman, 2013), Daniel Kahneman characterizes human thinking into two modes, through the lens of the dual process theory :\\n\\nFast thinking (System 1) operates quickly and automatically, driven by intuition and emotion while requiring little to no effort.\\nSlow thinking (System 2) demands deliberate, logical thought and significant cognitive efforts. This mode of thinking consumes more mental energy and requires intentional engagement.'),\n",
       " Document(id='614bfb68-0248-46f0-a48b-a6e668a26ca4', metadata={'source': 'https://lilianweng.github.io/posts/2024-11-28-reward-hacking/', 'title': \"Reward Hacking in Reinforcement Learning | Lil'Log\", 'description': 'Reward hacking occurs when a reinforcement learning (RL) agent exploits flaws or ambiguities in the reward function to achieve high rewards, without genuinely learning or completing the intended task. Reward hacking exists because RL environments are often imperfect, and it is fundamentally challenging to accurately specify a reward function.\\nWith the rise of language models generalizing to a broad spectrum of tasks and RLHF becomes a de facto method for alignment training, reward hacking in RL training of language models has become a critical practical challenge. Instances where the model learns to modify unit tests to pass coding tasks, or where responses contain biases that mimic a user’s preference, are pretty concerning and are likely one of the major blockers for real-world deployment of more autonomous use cases of AI models.', 'language': 'en'}, page_content=\"Language-Model\\nRlhf\\nAlignment\\nSafety\\nReinforcement-Learning\\nLong-Read\\n\\n\\n\\n« \\n\\nWhy We Think\\n\\n\\n »\\n\\nExtrinsic Hallucinations in LLMs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2025 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\"),\n",
       " Document(id='013e3c14-f6d6-4ee2-ba86-b81879bab93b', metadata={'source': 'https://lilianweng.github.io/posts/2025-05-01-thinking/', 'title': \"Why We Think | Lil'Log\", 'description': 'Special thanks to John Schulman for a lot of super valuable feedback and direct edits on this post.\\nTest time compute (Graves et al. 2016, Ling, et al. 2017, Cobbe et al. 2021) and Chain-of-thought (CoT) (Wei et al. 2022, Nye et al. 2021), have led to significant improvements in model performance, while raising many research questions. This post aims to review recent developments in how to effectively use test-time compute (i.e. “thinking time”) and why it helps.', 'language': 'en'}, page_content='Beam search decoding guided by LLM self-evaluation per reasoning step. (Image source: Xie et al. 2023)\\n\\nInterestingly, it is possible to trigger the emergent chain-of-thought reasoning paths without explicit zero-shot or few-shot prompting. Wang & Zhou (2024) discovered that if we branch out at the first sampling tokens by retaining the top $k$ tokens with highest confidence, measured as the difference between top-1 and top-2 candidates during sampling, and then continue these $k$ sampling trials with greedy decoding onward, many of these sequences natively contain CoT. Especially when CoT does appear in the context, it leads to a more confident decoding of the final answer. To calculate the confidence of the final answer, the answer span needs to be identified by task-specific heuristics (e.g. last numerical values for math questions) or  by prompting the model further with \"So the answer is\". The design choice of only branching out at the first token is based on the observation that early branching significantly enhances the diversity of potential paths, while later tokens are influenced a lot by previous sequences.\\n\\n\\nTop-$k$ decoding, $k$ refers to the number of candidates at the first sampling step. (Image source: Wang & Zhou, 2024)\\n\\nSequential Revision#\\nIf the model can reflect and correct mistakes in past responses, we would expect the model to produce a nice sequence of iterative revision with increasing quality. However, this self-correction capability turns out to not exist intrinsically among LLMs and does not easily work out of the box, due to various failure modes, such as, (1) hallucination, including modifying correct responses to be incorrect; (2) behavior collapse to non-correcting behavior; e.g. making minor or no modification on the first incorrect responses; or (3) fail to generalize to distribution shift at test time. Experiments by Huang et al. (2024) showed that naively applying self-correction leads to worse performance and external feedback is needed for models to self improve, which can be based on matching ground truths, heuristics and task-specific metrics, unit tests results for coding questions (Shinn, et al. 2023), a stronger model (Zhang et al. 2024), as well as human feedback (Liu et al. 2023).\\nSelf-correction learning (Welleck et al. 2023) aims to train a corrector model $P_\\\\theta(y \\\\mid y_0, x)$ given a fixed generator model $P_0(y_0 \\\\mid x)$. While the generator model remains to be generic, the corrector model can task-specific and only does generation conditioned on an initial model response and additional feedback (e.g. a sentence, a compiler trace, unit test results; can be optional):\\n\\nSelf-correction learning first generates first generates multiple outputs per prompt in the data pool;\\nthen create value-improving pairs by pairing two outputs for the same prompt together if one has a higher value than the other, (prompt $x$, hypothesis $y$, correction $y’$).\\nThese pairs are selected proportional to is improvement in value, $v(y’) - v(y)$, and similarity between two outputs, $\\\\text{Similarity}(y, y’)$ to train the corrector model.\\nTo encourage exploration, the corrector provides new generations into the data pool as well. At the inference time, the corrector can be used iteratively to create a correction trajectory of sequential revision.\\n\\n\\n\\nIllustration of self-correction learning by matching model outputs for the same problem to form value-improving pairs to train a correction model. (Image source: Welleck et al. 2023)'),\n",
       " Document(id='c6241df6-5f0e-42a0-8c64-f92948ba3d66', metadata={'source': 'https://lilianweng.github.io/posts/2025-05-01-thinking/', 'title': \"Why We Think | Lil'Log\", 'description': 'Special thanks to John Schulman for a lot of super valuable feedback and direct edits on this post.\\nTest time compute (Graves et al. 2016, Ling, et al. 2017, Cobbe et al. 2021) and Chain-of-thought (CoT) (Wei et al. 2022, Nye et al. 2021), have led to significant improvements in model performance, while raising many research questions. This post aims to review recent developments in how to effectively use test-time compute (i.e. “thinking time”) and why it helps.', 'language': 'en'}, page_content='[42] Sachin Goyal et al. “Think before you speak: Training Language Models With Pause Tokens.”. ICLR 2024.\\n[43] Eric Zelikman, et al. “Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking.”. arXiv preprint arXiv:2403.09629 (2025).\\n[44] Wangchunshu Zhou et al. “Towards Interpretable Natural Language Understanding with Explanations as Latent Variables.”. NeurIPS 2020.\\n[45] Du Phan et al. “Training Chain-of-Thought via Latent-Variable Inference.”. NeurIPS 2023.\\n[46] Yangjun Ruan et al. “Reasoning to Learn from Latent Thoughts.”. arXiv preprint arXiv:2503.18866 (2025).\\n[47] Xuezhi Wang et al. “Rationale-Augmented Ensembles in Language Models.”. arXiv preprint arXiv:2207.00747 (2022).\\n[48] Jared Kaplan, et al. “Scaling Laws for Neural Language Models.”. arXiv preprint arXiv:2001.08361 (2020).\\n[49] Niklas Muennighoff & Zitong Yang, et al. “s1: Simple test-time scaling.”. arXiv preprint arXiv:2501.19393 (2025).\\n[50] Peiyi Wang, et al. “Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations” arXiv preprint arXiv:2312.08935 (2023).\\n[51] Yixin Liu, et al. “Improving Large Language Model Fine-tuning for Solving Math Problems.” arXiv preprint arXiv:2310.10047 (2023).\\n[52] Charlie Snell, et al. “Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters.”. arXiv preprint arXiv:2408.03314 (2024).\\n[53] OpenAI. o1-preview: “Learning to reason with LLMs.” Sep 12, 2024.\\n[54] OpenAI. o3: “Introducing OpenAI o3 and o4-mini.” Apr 16, 2025.')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is Chain of thought?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_blog_posts\",\n",
    "    \"Search and return information about Lilian Weng blog posts\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ve generalization results of a model trained with expert iteration according '\n",
      " 'to our curriculum from each stage to the next. (Image source: Denison et al. '\n",
      " '2024)\\n'\n",
      " '\\n'\n",
      " 'It is noteworthy that even after the curriculum, the model overwrote the '\n",
      " 'reward and avoided detection less than 1/1000 of the time. Even when a model '\n",
      " 'was trained on curricula which directly incentivized reward hacking, the '\n",
      " 'model overwrote their reward less than 1% of the time and hacked unit tests '\n",
      " 'even less often. As a simple mitigation, supervised fine-tuning the model on '\n",
      " 'the first two environments–where the reward hacking behavior is easy to be '\n",
      " 'detected (sycophancy and flattery)—with SFT data that does not game the env '\n",
      " 'was found to reduce the likelihood of reward tampering in holdout '\n",
      " 'environments.\\n'\n",
      " 'Peek into Mitigations#\\n'\n",
      " 'While there is a large body of literature discussing the phenomenon of '\n",
      " 'reward hacking, there has been not a ton of work on mitigations for reward '\n",
      " 'hacking, especially in the area of RLHF and LLMs. Let')\n"
     ]
    }
   ],
   "source": [
    "# Test the query tool\n",
    "result = retriever_tool.invoke({\"query\": \"types of reward hacking\"})\n",
    "pprint(result[10:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"google/gemini-2.5-flash-lite\", temperature=0, max_tokens=2048)\n",
    "tools = [retriever_tool]\n",
    "# Bind tools to LLM for agent functionality\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RAG agent system prompt\n",
    "rag_prompt = \"\"\"You are a helpful assistant tasked with retrieving information from a series of technical blog posts by Lilian Weng.\n",
    "Clarify the scope of research with the user before using your retrieval tool to gather context. Reflect on any context you fetch, and\n",
    "proceed until you have sufficient context to answer the user's research request.\"\"\"\n",
    "\n",
    "\n",
    "def llm_call(state: MessagesState):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            llm_with_tools.invoke(\n",
    "                [SystemMessage(content=rag_prompt)] + state[\"messages\"]\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "agent_builder = StateGraph(MessagesState)\n",
    "\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"tool_call\", ToolNode(tools))\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tool_call\": \"tool_call\",\n",
    "        \"__end__\": END,\n",
    "    },\n",
    ")\n",
    "agent_builder.add_edge(\"tool_call\", \"llm_call\")\n",
    "agent_builder.set_entry_point(\"llm_call\")\n",
    "agent = agent_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">╭─────────────────────────────────────────────────── 🧑 Human ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> What are the types of reward hacking discussed in the blogs?                                                    <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m╭─\u001b[0m\u001b[34m──────────────────────────────────────────────────\u001b[0m\u001b[34m 🧑 Human \u001b[0m\u001b[34m───────────────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m What are the types of reward hacking discussed in the blogs?                                                    \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╭───────────────────────────────────────────────────── 📝 AI ─────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> I can help you with that! I'll search Lilian Weng's blog posts for discussions on reward hacking.               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> What specific aspects of reward hacking are you most interested in? For example, are you looking for:           <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> *   **Types of reward hacking:** (e.g., specification gaming, overfitting)                                      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> *   **Examples of reward hacking:** in different domains (e.g., games, robotics)                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> *   **Mitigation strategies:** for reward hacking                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> *   **The underlying causes:** of reward hacking                                                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> The more specific you are, the better I can tailor the search results for you. If you're unsure, I can start    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> with a general search for \"reward hacking.\"                                                                     <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37m╭─\u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m 📝 AI \u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m─╮\u001b[0m\n",
       "\u001b[37m│\u001b[0m I can help you with that! I'll search Lilian Weng's blog posts for discussions on reward hacking.               \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m What specific aspects of reward hacking are you most interested in? For example, are you looking for:           \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m *   **Types of reward hacking:** (e.g., specification gaming, overfitting)                                      \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m *   **Examples of reward hacking:** in different domains (e.g., games, robotics)                                \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m *   **Mitigation strategies:** for reward hacking                                                               \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m *   **The underlying causes:** of reward hacking                                                                \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m The more specific you are, the better I can tailor the search results for you. If you're unsure, I can start    \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m with a general search for \"reward hacking.\"                                                                     \u001b[37m│\u001b[0m\n",
       "\u001b[37m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What are the types of reward hacking discussed in the blogs?\"\n",
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n",
    "\n",
    "# Format and display results\n",
    "format_messages(result[\"messages\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vincent": {
   "sessionId": "f2a0995d41cb371f6db09d73_2025-08-09T16-27-00-112Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
