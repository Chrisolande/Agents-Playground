{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from fastembed import TextEmbedding\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import END, MessagesState, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from dataclasses import dataclass, field\n",
    "from langchain_core.embeddings import Embeddings\n",
    "import os\n",
    "\n",
    "from utils import format_messages\n",
    "\n",
    "# _ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2025-05-01-thinking/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = [item for sublist in docs for item in sublist]\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=50\n",
    ")\n",
    "\n",
    "# Splits documents into chunks\n",
    "doc_splits = text_splitter.split_documents(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FastEmbed(Embeddings):\n",
    "    fe: TextEmbedding = field(default_factory=TextEmbedding)\n",
    "\n",
    "    def embed_documents(self, documents: list[str]) -> list[list[float]]:\n",
    "        return [emb.tolist() for emb in self.fe.embed(documents)]\n",
    "\n",
    "    def embed_query(self, query: str) -> list[float]:\n",
    "        return list(self.fe.embed([query]))[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af45a03da6be4317aec9a96dcb605a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f2748d6993466e98e7d0ff6c2b1415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d72124ef9294c22b141f7daaaa3a4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3859c1553d4108beb47b02de2718ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0c9fdad6b648c3aa6bfb68070dba03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d75ec70703f47dc9b65feaec67fab61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_fp16.onnx:   0%|          | 0.00/321M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = FastEmbed(\n",
    "    TextEmbedding(\n",
    "        model_name=\"jinaai/jina-embeddings-v2-base-de\",\n",
    "        cache_dir=os.path.expanduser(\"~/.cache/fastembed\"),\n",
    "    )\n",
    ")\n",
    "vector_store = InMemoryVectorStore.from_documents(\n",
    "    documents=doc_splits, embedding=embeddings\n",
    ")\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='3562d8cb-5a4e-43de-a8a1-935d315fdb96', metadata={'source': 'https://lilianweng.github.io/posts/2025-05-01-thinking/', 'title': \"Why We Think | Lil'Log\", 'description': 'Special thanks to John Schulman for a lot of super valuable feedback and direct edits on this post.\\nTest time compute (Graves et al. 2016, Ling, et al. 2017, Cobbe et al. 2021) and Chain-of-thought (CoT) (Wei et al. 2022, Nye et al. 2021), have led to significant improvements in model performance, while raising many research questions. This post aims to review recent developments in how to effectively use test-time compute (i.e. “thinking time”) and why it helps.', 'language': 'en'}, page_content='[20] Yuxi Xie, et al. “Self-Evaluation Guided Beam Search for Reasoning.”. NeurIPS 2023.\\n[21] Yangzhen Wu, et al. “Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models”. ICLR 2025.\\n[22] Dongwei Jiang, et al. “RATIONALYST: Pre-training Process-Supervision for Improving Reasoning”. arXiv preprint arXiv:2410.01044 (2024).\\n[23] Xuezhi Wang and Denny Zhou. “Chain-of-Thought Reasoning Without Prompting.”. arXiv preprint arXiv:2402.10200 (2024).\\n[24] DeepSeek-AI. “DeepSeek-V3 Technical Report.” arXiv preprint arXiv:2412.19437 (2024).\\n[25] DeepSeek-AI. “DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.”. arXiv preprint arXiv:2501.12948 (2025).\\n[26] Luyu Gao, Aman Madaan & Shuyan Zhou, et al. “PAL: Program-aided Language Models.”. ICML 2023.\\n[27] Shunyu Yao, et al. “ReAct: Synergizing Reasoning and Acting in Language Models.”. ICLR 2023.\\n[29] Bowen Baker, et al. “Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation.”. arXiv preprint arXiv:2503.11926 (2025).\\n[30] Wojciech Zaremba, et al. “Trading Inference-Time Compute for Adversarial Robustness.”. arXiv preprint arXiv:2501.18841 (2025).\\n[31] Tamera Lanham, et al. “Measuring Faithfulness in Chain-of-Thought Reasoning”. arXiv preprint arXiv:2307.13702 (2023).\\n[32] Boshi Wang, et al. “Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters.”. ACL 2023.\\n[33] Miles Turpin, et al. “Language Models Don’t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting.”. NeuriPS 2023.\\n[34] James Chua & Owain Evans. “Are DeepSeek R1 And Other Reasoning Models More Faithful?”. arXiv preprint arXiv:2501.08156 (2025).\\n[35] Yanda Chen et al. “Reasoning Models Don’t Always Say What They Think”. arXiv preprint arXiv:2505.05410 (2025).\\n[36] Edward Yeo, et al. “Demystifying Long Chain-of-Thought Reasoning in LLMs.”. arXiv preprint arXiv:2502.03373 (2025).\\n[37] Mostafa Dehghani, et al. “Universal Transformers.”. ICLR 2019.\\n[38] DeLesley Hutchins, et al. “Block-Recurrent Transformers.”. NeurIPS 2022.\\n[39] Aydar Bulatov, et al. “Recurrent Memory Transformers.”. NeuriPS 2022.\\n[40] Jonas Geiping, et al. “Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach.”. arXiv preprint arXiv:2502.05171 (2025).\\n[41] Herel & Mikolov. “Thinking Tokens for Language Modeling.”. AITP 2023.\\n[42] Sachin Goyal et al. “Think before you speak: Training Language Models With Pause Tokens.”. ICLR 2024.'),\n",
       " Document(id='2318bafe-ae15-4771-abca-8a23bfae7ac0', metadata={'source': 'https://lilianweng.github.io/posts/2025-05-01-thinking/', 'title': \"Why We Think | Lil'Log\", 'description': 'Special thanks to John Schulman for a lot of super valuable feedback and direct edits on this post.\\nTest time compute (Graves et al. 2016, Ling, et al. 2017, Cobbe et al. 2021) and Chain-of-thought (CoT) (Wei et al. 2022, Nye et al. 2021), have led to significant improvements in model performance, while raising many research questions. This post aims to review recent developments in how to effectively use test-time compute (i.e. “thinking time”) and why it helps.', 'language': 'en'}, page_content='Citation#\\nPlease cite this work as:\\nWeng, Lilian. \"Why We Think\". Lil\\'Log (May 2025). https://lilianweng.github.io/posts/2025-05-01-thinking/\\nOr use the BibTex citation:\\n@article{weng2025think,\\n  title = {Why We Think},\\n  author = {Weng, Lilian},\\n  journal = {lilianweng.github.io},\\n  year = {2025},\\n  month = {May},\\n  url = \"https://lilianweng.github.io/posts/2025-05-01-thinking/\"\\n}\\nReferences#\\n[1] Alex Graves. “Adaptive Computation Time for Recurrent Neural Networks.”. arXiv preprint arXiv:1603.08983 (2016).\\n[2] Wang Ling, et al. “Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems.”. arXiv preprint arXiv:1705.04146 (2017).\\n[3] Karl Cobbe, et al. “Training Verifiers to Solve Math Word Problems.”. arXiv preprint arXiv:2110.14168 (2021).\\n[4] Jason Wei, et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.”. NeurIPS 2022.\\n[5] Maxwell Nye, et al. “Show Your Work: Scratchpads for Intermediate Computation with Language Models.”. arXiv preprint arXiv:2112.00114 (2021).\\n[6] Daniel Kahneman. Thinking, Fast and Slow. Farrar, Straus and Giroux (2013).\\n[7] Takeshi Kojima, et al. “Large Language Models are Zero-Shot Reasoners.”. NeurIPS 2022.\\n[8] Michihiro Yasunaga, et al. “Large Language Models as Analogical Reasoners”. arXiv preprint arXiv:2310.01714 (2023).\\n[9] Eric Zelikman, et al. “STaR: Bootstrapping Reasoning With Reasoning.”. NeurIPS 2022.\\n[10] Xuezhi Wang, et al. “Self-consistency Improves Chain of Thought Reasoning in Language Models.”. ACL 2023.\\n[11] Ryo Kamoi, et al. “When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs.”. TACL 2024.\\n[12] Jie Huang, et al. “Large Language Models Cannot Self-Correct Reasoning Yet.”. ICLR 2024.\\n[13] Noah Shinn, et al. “Reflexion: Language Agents with Verbal Reinforcement Learning.”. arXiv preprint arXiv:2303.11366 (2023).\\n[14] Yunxiang Zhang, et al. “Small Language Models Need Strong Verifiers to Self-Correct Reasoning.”. ACL Findings 2024.\\n[15] Hao Liu, et al. “Chain of Hindsight Aligns Language Models with Feedback.”. arXiv preprint arXiv:2302.02676 (2023).\\n[16] Sean Welleck, et al. “Generating Sequences by Learning to Self-Correct.”. arXiv preprint arXiv:2211.00053 (2023).\\n[17] Yuxiao Qu, et al. “Recursive Introspection: Teaching Language Model Agents How to Self-Improve.”. arXiv preprint arXiv:2407.18219 (2024).\\n[18] Aviral Kumar, et al. “Training Language Models to Self-Correct via Reinforcement Learning.”. arXiv preprint arXiv:2409.12917 (2024).\\n[19] Hunter Lightman, et al. “Let’s Verify Step by Step.”. arXiv preprint arXiv:2305.20050 (2023).\\n[20] Yuxi Xie, et al. “Self-Evaluation Guided Beam Search for Reasoning.”. NeurIPS 2023.'),\n",
       " Document(id='24b26a59-50ed-4c34-8fab-625fa25a238e', metadata={'source': 'https://lilianweng.github.io/posts/2025-05-01-thinking/', 'title': \"Why We Think | Lil'Log\", 'description': 'Special thanks to John Schulman for a lot of super valuable feedback and direct edits on this post.\\nTest time compute (Graves et al. 2016, Ling, et al. 2017, Cobbe et al. 2021) and Chain-of-thought (CoT) (Wei et al. 2022, Nye et al. 2021), have led to significant improvements in model performance, while raising many research questions. This post aims to review recent developments in how to effectively use test-time compute (i.e. “thinking time”) and why it helps.', 'language': 'en'}, page_content='Why We Think | Lil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Why We Think\\n    \\nDate: May 1, 2025  |  Estimated Reading Time: 40 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nMotivation\\n\\nAnalogy to Psychology\\n\\nComputation as a Resource\\n\\nLatent Variable Modeling\\n\\n\\nThinking in Tokens\\n\\nBranching and Editing\\n\\nParallel Sampling\\n\\nSequential Revision\\n\\n\\nRL for Better Reasoning\\n\\nExternal Tool Use\\n\\nThinking Faithfully\\n\\nDoes the Model Tell What it Thinks Faithfully\\n\\nOptimization Pressure on CoT: Good or Bad?\\n\\n\\n\\nThinking in Continuous Space\\n\\nRecurrent Architecture\\n\\nThinking Tokens\\n\\n\\nThinking as Latent Variables\\n\\nExpectation-Maximization\\n\\nIterative Learning\\n\\n\\nScaling Laws for Thinking Time\\n\\nWhat’s for Future\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nSpecial thanks to John Schulman for a lot of super valuable feedback and direct edits on this post.\\nTest time compute (Graves et al. 2016, Ling, et al. 2017, Cobbe et al. 2021) and Chain-of-thought (CoT) (Wei et al. 2022, Nye et al. 2021), have led to significant improvements in model performance, while raising many research questions. This post aims to review recent developments in how to effectively use test-time compute (i.e. “thinking time”) and why it helps.\\nMotivation#\\nEnabling models to think for longer can be motivated in a few different ways.\\nAnalogy to Psychology#\\nThe core idea is deeply connected to how humans think. We humans cannot immediately provide the answer for \"What\\'s 12345 times 56789?\". Rather, it is natural to spend time pondering and analyzing before getting to the result, especially for complex problems. In Thinking, Fast and Slow (Kahneman, 2013), Daniel Kahneman characterizes human thinking into two modes, through the lens of the dual process theory :\\n\\nFast thinking (System 1) operates quickly and automatically, driven by intuition and emotion while requiring little to no effort.\\nSlow thinking (System 2) demands deliberate, logical thought and significant cognitive efforts. This mode of thinking consumes more mental energy and requires intentional engagement.'),\n",
       " Document(id='2bead899-e9fb-4597-9ba8-52dfb049abcb', metadata={'source': 'https://lilianweng.github.io/posts/2025-05-01-thinking/', 'title': \"Why We Think | Lil'Log\", 'description': 'Special thanks to John Schulman for a lot of super valuable feedback and direct edits on this post.\\nTest time compute (Graves et al. 2016, Ling, et al. 2017, Cobbe et al. 2021) and Chain-of-thought (CoT) (Wei et al. 2022, Nye et al. 2021), have led to significant improvements in model performance, while raising many research questions. This post aims to review recent developments in how to effectively use test-time compute (i.e. “thinking time”) and why it helps.', 'language': 'en'}, page_content='Plot of an experiment on training a 3.5B model with depth recurrence. The saturation roughly happens around $\\\\bar{r} = 32$, making us wonder how this architecture extrapolate and generalize to larger iteration counts. (Image source: Geiping et al. 2025)\\n\\nThinking Tokens#\\nThinking tokens refer to a set of implicit tokens introduced during training or inference that do not carry direct linguistic meaning. Instead, their role is to provide extra thinking time and compute power for the model to perform better.\\nHerel & Mikolov (2023) introduced the idea of inserting special thinking tokens (<T>) after each word in a sentence and training the model on such a dataset. Each thinking token buys extra time for the model to process and make better predictions. Training with thinking tokens on a toy model setup results in lower perplexity than baseline model trained without them. The benefits of thinking tokens are more pronounced for non-trivial reasoning tasks or sentences involving numbers.\\nSimilarly, pause tokens proposed by Goyal et al. (2024) delay the model’s outputs by appending dummy tokens (e.g. character like . or #) at the end of the input sequence, giving the model extra computation during inference. It is important to inject such pause tokens both during training and inference time, while only fine-tuning on pause tokens leads to limited gain. During training, multiple copies of pause tokens are inserted at uniformly random locations and the loss on pause tokens is ignored for training.\\n\\n\\nIllustration of how pause tokens are injected during training and inference in comparison to standard setup. (Image source: Goyal et al. 2024)\\n\\nInterestingly, thinking tokens or pause tokens in above experiments do not carry any extra information or add many new parameters. But why is it still helpful? On one hand, it helps expand the computation by introducing more inference loops, effectively increasing computational capacity. On the other hand, it can be seen as acting as a special, implicit form of CoTs. A drawback here is hat the model needs to be pretrained with respect to thinking tokens. Still, this strategy is an interesting way to further improve the capability of test time compute utilization on top of inference time CoTs.\\nQuiet-STaR (Zelikman et al. 2025) introduces token-level reasoning by training the model to generate rationales after every token to explain future text. It mixes the future-text predictions with and without rationales and uses learning to generate better rationales and uses REINFORCE to optimize the quality of rationale generation.\\n\\n\\nIllustration of Quiet-STaR. (Image source: Zelikman et al. 2025)\\n\\nQuiet-STaR consists of three stages:\\n\\n\\nThink: Predicting next tokens with rationales. Due to high computational cost demanded by token level reasoning, this process is designed to generate multiple rationales in parallel. A special attention map is used to enable all thought tokens to only pay attention to themselves, all preceding thought tokens within the same thought, and the preceding text.\\n\\n\\nTalk: Next token prediction without rationale is mixed with post-rationale prediction. The mixing weight for two logits is learned by a special mixing head of a shallow MLP out of hidden output after each rationale. Correct next tokens can be selected via teacher forcing.\\n\\n\\nLearn: Train the model to generate better rationale via REINFORCE by learning from examples that increase the probability of correct next token while discarding those that hurt the prediction.\\n\\n\\nWithout dataset specific fine-tuning, Quiet-STaR improves zero-shot results on CommonsenseQA (36.3%→47.2%) and GSM8K (5.9%→10.9%), within experiments on Mistral 7B.\\nThinking as Latent Variables#\\nA latent variable model defines a probabilistic framework where observable data is explained through unobserved (latent) variables. These latent variables capture hidden structures or intermediate processes that generate the observable outcomes. Language models can be viewed as probabilistic latent variable models where test-time thinking and reasoning steps are latent thought variables (Zhou et al. 2020, Phan et al. 2023). Such a latent variable model defines a joint distribution of problems x_i, answers y_i and latent thought z_i. We would like to optimize the log-likelihood of answers given questions and a variety of CoTs as latent variables (N is the number of samples; $K$ is the number of CoTs per problem):')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is Chain of thought?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_blog_posts\",\n",
    "    \"Search and return information about Lilian Weng blog posts\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('king examples in real life#\\n'\n",
      " '\\n'\n",
      " 'The recommendation algorithm for social media is intended to provide useful '\n",
      " 'information. However, usefulness is often measured by proxy metrics, such as '\n",
      " 'the number of likes or comments, or the time or frequency of engagement on '\n",
      " 'the platform. The algorithm ends up recommending content that can affect '\n",
      " 'users’ emotion states such as outrageous and extreme content in order to '\n",
      " 'trigger more engagement. (Harari, 2024)\\n'\n",
      " 'Optimizing for misspecified proxy metrics for a video sharing site may '\n",
      " 'aggressively increase the watch time of users while the true goal is to '\n",
      " 'optimize users’ subjective well-being. (Link)\\n'\n",
      " '“The Big Short” - 2008 financial crisis caused by the housing bubble. Reward '\n",
      " 'hacking of our society happened as people tried to game the financial '\n",
      " 'system.\\n'\n",
      " '\\n'\n",
      " 'Why does Reward Hacking Exist?#\\n'\n",
      " 'Goodhart’s Law states that “When a measure becomes a target, it ceases to be '\n",
      " 'a good measure”. The intuition is that a good metric can become corrupted '\n",
      " 'once significant pr')\n"
     ]
    }
   ],
   "source": [
    "# Test the query tool\n",
    "result = retriever_tool.invoke({\"query\": \"types of reward hacking\"})\n",
    "pprint(result[10:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatDeepSeek(model=\"deepseek-chat\", temperature=1.3)\n",
    "tools = [retriever_tool]\n",
    "# Bind tools to LLM for agent functionality\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RAG agent system prompt\n",
    "rag_prompt = \"\"\"You are a helpful assistant tasked with retrieving information from a series of technical blog posts by Lilian Weng.\n",
    "Clarify the scope of research with the user before using your retrieval tool to gather context. Reflect on any context you fetch, and\n",
    "proceed until you have sufficient context to answer the user's research request.\"\"\"\n",
    "\n",
    "\n",
    "def llm_call(state: MessagesState):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            llm_with_tools.invoke(\n",
    "                [SystemMessage(content=rag_prompt)] + state[\"messages\"]\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "agent_builder = StateGraph(MessagesState)\n",
    "\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"tool_call\", ToolNode(tools))\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\": \"tool_call\",\n",
    "        \"__end__\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "agent_builder.add_edge(\"tool_call\", \"llm_call\")\n",
    "agent_builder.set_entry_point(\"llm_call\")\n",
    "agent = agent_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">╭─────────────────────────────────────────────────── 🧑 Human ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> What are the types of reward hacking discussed in the blogs?                                                    <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m╭─\u001b[0m\u001b[34m──────────────────────────────────────────────────\u001b[0m\u001b[34m 🧑 Human \u001b[0m\u001b[34m───────────────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m What are the types of reward hacking discussed in the blogs?                                                    \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╭───────────────────────────────────────────────────── 📝 AI ─────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> I'll help you find information about reward hacking types discussed in Lilian Weng's blog posts. Let me search  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> for relevant content.                                                                                           <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37m╭─\u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m 📝 AI \u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m─╮\u001b[0m\n",
       "\u001b[37m│\u001b[0m I'll help you find information about reward hacking types discussed in Lilian Weng's blog posts. Let me search  \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m for relevant content.                                                                                           \u001b[37m│\u001b[0m\n",
       "\u001b[37m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">╭──────────────────────────────────────────────── 🔧 Tool Output ─────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward hacking examples in real life#                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The recommendation algorithm for social media is intended to provide useful information. However, usefulness is <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> often measured by proxy metrics, such as the number of likes or comments, or the time or frequency of           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> engagement on the platform. The algorithm ends up recommending content that can affect users’ emotion states    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> such as outrageous and extreme content in order to trigger more engagement. (Harari, 2024)                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Optimizing for misspecified proxy metrics for a video sharing site may aggressively increase the watch time of  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> users while the true goal is to optimize users’ subjective well-being. (Link)                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> “The Big Short” - 2008 financial crisis caused by the housing bubble. Reward hacking of our society happened as <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> people tried to game the financial system.                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Why does Reward Hacking Exist?#                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Goodhart’s Law states that “When a measure becomes a target, it ceases to be a good measure”. The intuition is  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> that a good metric can become corrupted once significant pressure is applied to optimize it. It is challenging  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> to specify a 100% accurate reward objective and any proxy suffers the risk of being hacked, as RL algorithm     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> exploits any small imperfection in the reward function definition. Garrabrant (2017) categorized Goodhart’s law <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> into 4 variants:                                                                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Regressional - selection for an imperfect proxy necessarily also selects for noise.                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Extremal - the metric selection pushes the state distribution into a region of different data distribution.     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Causal -  when there is a non-causal correlation between the proxy and the goal, intervening on the proxy may   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> fail to intervene on the goal.                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Adversarial - optimization for a proxy provides an incentive for adversaries to correlate their goal with the   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> proxy.                                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Amodei et al. (2016) summarized that reward hacking, mainly in RL setting, may occur due to:                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Partial observed states and goals are imperfect representation of the environment status.                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The system itself is complex and susceptible to hacking; e.g., if the agent is allowed to execute code that     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> changes part of the environment, it becomes much easier to exploit the environment’s mechanisms.                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The reward may involve abstract concept that is hard to be learned or formulated; e.g., a reward function with  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> high-dimensional inputs may disproportionately rely on a few dimensions.                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RL targets to get the reward function highly optimized, so there exists an intrinsic “conflict”, making the     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> design of good RL objective challenging. A special case is a type of the reward function with a                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> self-reinforcing feedback component, where the reward may get amplified and distorted to a point that breaks    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> down the original intent, such as an ads placement algorithm leading to winners getting all.                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Besides, identifying the exact reward function for which an optimal agent optimizes its behavior is in general  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> impossible since there could be an infinite number of reward functions consistent with any observed policy in   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> an fixed environment (Ng &amp; Russell, 2000). Amin and Singh (2016) separated the causes of this unidentifiability <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> into two classes:                                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Representational - a set of reward functions is behaviorally invariant under certain arithmetic operations      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (e.g., re-scaling)                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Experimental - $\\pi$’s observed behavior is insufficient to distinguish between two or more reward functions    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> which both rationalize the behavior of the agent (the behavior is optimal under both)                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward Hacking in Reinforcement Learning | Lil'Log                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Lil'Log                                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> |                                                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Posts                                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Archive                                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Search                                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Tags                                                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> FAQ                                                                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>       Reward Hacking in Reinforcement Learning                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Date: November 28, 2024  |  Estimated Reading Time: 37 min  |  Author: Lilian Weng                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Table of Contents                                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Background                                                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward Function in RL                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Spurious Correlation                                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Let’s Define Reward Hacking                                                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> List of Examples                                                                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward hacking examples in RL tasks                                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward hacking examples in LLM tasks                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward hacking examples in real life                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Why does Reward Hacking Exist?                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Hacking RL Environment                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Hacking RLHF of LLMs                                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Hacking the Training Process                                                                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Hacking the Evaluator                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> In-Context Reward Hacking                                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Generalization of Hacking Skills                                                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Peek into Mitigations                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RL Algorithm Improvement                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Detecting Reward Hacking                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Data Analysis of RLHF                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Citation                                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> References                                                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward hacking occurs when a reinforcement learning (RL) agent exploits flaws or ambiguities in the reward      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> function to achieve high rewards, without genuinely learning or completing the intended task. Reward hacking    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> exists because RL environments are often imperfect, and it is fundamentally challenging to accurately specify a <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> reward function.                                                                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> With the rise of language models generalizing to a broad spectrum of tasks and RLHF becomes a de facto method   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> for alignment training, reward hacking in RL training of language models has become a critical practical        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> challenge. Instances where the model learns to modify unit tests to pass coding tasks, or where responses       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> contain biases that mimic a user’s preference, are pretty concerning and are likely one of the major blockers   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> for real-world deployment of more autonomous use cases of AI models.                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Most of the past work on this topic has been quite theoretical and focused on defining or demonstrating the     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> existence of reward hacking. However, research into practical mitigations, especially in the context of RLHF    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> and LLMs, remains limited. I especially want to call out for more research efforts directed toward              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> understanding and developing mitigation for reward hacking in the future. Hope I will be able to cover the      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> mitigation part in a dedicated post soon.                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Background#                                                                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward Function in RL#                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward function defines the task, and reward shaping significantly impacts learning efficiency and accuracy in  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> reinforcement learning. Designing a reward function for an RL task often feels like a ‘dark art’. Many factors  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> contribute to this complexity: How you decompose a big goal into small goals? Is the reward sparse or dense?    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> How you measure the success? Various choices may lead to good or problematic learning dynamics, including       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> unlearnable tasks or hackable reward functions. There is a long history of research on how to do reward shaping <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> in RL.                                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> For example, in an 1999 paper by Ng et al., the authors studied how to modify the reward function in Markov     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Decision Processes (MDPs) such that the optimal policy remains unchanged. They found that linear transformation <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> works. Given a MDP $M = (S, A, T, \\gamma, R)$, we want to create a transformed MDP $M’ = (S, A, T, \\gamma, R’)$ <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> where $R’ = R + F$ and $F: S \\times A \\times S \\mapsto \\mathbb{R}$, such that we can guide the learning         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> algorithm to be more efficient. Given a real-valued function $\\Phi: S \\mapsto \\mathbb{R}$, $F$ is a             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> potential-based shaping function if for all $s \\in S - {s_0}, a \\in A, s’ \\in S$:                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> F(s, a, s') = \\gamma \\Phi(s') - \\Phi(s)                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Quantitative generalization results of a model trained with expert iteration according to our curriculum from   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> each stage to the next. (Image source: Denison et al. 2024)                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> It is noteworthy that even after the curriculum, the model overwrote the reward and avoided detection less than <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> 1/1000 of the time. Even when a model was trained on curricula which directly incentivized reward hacking, the  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> model overwrote their reward less than 1% of the time and hacked unit tests even less often. As a simple        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> mitigation, supervised fine-tuning the model on the first two environments–where the reward hacking behavior is <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> easy to be detected (sycophancy and flattery)—with SFT data that does not game the env was found to reduce the  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> likelihood of reward tampering in holdout environments.                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Peek into Mitigations#                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> While there is a large body of literature discussing the phenomenon of reward hacking, there has been not a ton <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> of work on mitigations for reward hacking, especially in the area of RLHF and LLMs. Let’s lightly review three  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> potential approaches in this section, not exhaustive yet.                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RL Algorithm Improvement#                                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Amodei et al. (2016) pointed out some directions for mitigating reward hacking in RL training:                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Adversarial reward functions. We treat the reward function as an adaptive agent itself and it can adapt to new  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> tricks that the model discovered where the reward is high but human rating is low.                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Model lookahead. It is possible to give reward based on future anticipated states; e.g., if the agent is gonna  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> replace the reward function, it gets negative rewards.                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Adversarial blinding. We can blind the model with certain variables such that the agent cannot learn            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> information that enables it to hack the reward function.                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Careful engineering. Some types of reward hacking against the system design can be avoided by careful           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> engineering; e.g., sandboxing the agent to isolate its actions from its reward signals.                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward capping. This strategy is to simply limit the maximum possible reward, as it can effectively prevent     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> rare events of the agent hacking to get a super high pay-off strategy.                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Counterexample resistance. Improvement on adversarial robustness should benefit the robustness of the reward    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> function.                                                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Combination of multiple rewards. Combining different types of rewards could make it harder to be hacked.        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward pretraining. We can learn a reward function from a collection of (state, reward) samples, but depending  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> on how well this supervised training setup is, it may come with other baggages. RLHF depends on this but        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> learned scalar reward models are quite vulnerable to learning undesired traits.                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Variable indifference. The goal is to ask the agent to optimize some variables in the environment but not       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> others.                                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Trip wires. We can intentionally introduce some vulnerabilities and set up monitoring and alerts if any gets    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> reward hacked.                                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> In RL setups where human feedback is formed as approval of agent actions, Uesato et al. (2020) proposed to      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> prevent reward tampering with decoupled approval.  If the feedback is conditioned on $(s, a)$ (state, action),  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> we can never get uncorrupted feedback for action $a$ at state $s$ once reward tampering happens for this pair.  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Decoupling means that the query action for collecting feedback is sampled independently from the action taken   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> in the world. Feedback is received even before the action is executed in the world, thus preventing the action  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> from corrupting its own feedback.                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Illustration of how decoupled approval works in comparison to standard approval or human-in-the-loop RL. (Image <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> source: Uesato et al. 2020)                                                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> With decoupled approval, the action (taken in the world) and the query (for getting user approval feedback) are <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> sampled independently. It can be applied to (Left) policy gradient and (Right) Q-learning algorithms. (Image    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> source: Uesato et al. 2020)                                                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Detecting Reward Hacking#                                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> An alternative mitigation is to detect reward hacking by framing it as an anomaly detection task, where the     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> detector (“a trusted policy” with trajectories and rewards validated by human) should flag instances of         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> misalignment (Pan et al. 2022). Given (1) a trusted policy and (2) a collection of manually labeled trajectory  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> rollouts, we can build a binary classifier based on distances between action distribution of two policies, the  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> trusted policy and the target policy, and measure the accuracy of this anomaly detection classifier. In         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> experiments by Pan et al. (2022), they observed that different detectors are better for different tasks and     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> none of the tested classifier can achieve AUROC greater than 60% across all tested RL environments.             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Performance of detectors on different tasks. (Image source: Pan et al. 2022)                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Hacking RL Environment#                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward hacking is expected to be a more common problem as the model and the algorithm become increasingly       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> sophisticated. A more intelligent agent is more capable of finding “holes” in the design of reward function and <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> exploiting the task specification—in other words, achieving higher proxy rewards but lower true rewards. By     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> contrast, a weaker algorithm may not be able to find such loopholes, and thus we would not observe any reward   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> hacking or identify issues in the current reward function design when the model is not strong enough.           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> In a set of zero-sum robotics self-play games (Bansal et al., 2017), we can train two agents (victim vs.        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> opponent) to compete against each other. A standard training process produces a victim agent with adequate      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> performance when playing against a normal opponent. However, it is easy to train an adversarial opponent policy <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> that can defeat the victim reliably despite outputting seemingly random actions and training with fewer than 3% <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> of time steps (Gleave et al., 2020). Training of adversarial policies involves optimizing the sum of discounted <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> rewards, as in standard RL setup, while treating the victim policy as a black-box model.                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> An intuitive way to mitigate adversarial policies attacks is to fine-tune victims against adversarial policies. <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> However, the victim remains vulnerable to new versions of adversarial policies once retrained against the new   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> victim policy.                                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Why does adversarial policy exist? The hypothesis is that adversarial policies introduce OOD observations to    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the victim rather than physically interfering with it. Evidence shows that when the victim’s observation of the <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> opponent’s position is masked and set to a static state, the victim becomes more robust to adversaries,         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> although performing worse against a normal opponent policy. Furthermore, a higher-dimensional observation space <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> enhances performance under normal circumstances but makes the policy more vulnerable to adversarial opponents.  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Pan et al. (2022) investigated reward hacking as a function of agent capabilities, including (1) model size,    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (2) action space resolution, (3) observation space noise, and (4) training time. They also proposed a taxonomy  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> of three types of misspecified proxy rewards:                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Misweighting: Proxy and true rewards capture the same desiderata, but differ in their relative importance.      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Ontological: Proxy and true rewards use different desiderata to capture the same concept.                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Scope: The proxy measures desiderata over a restricted domain (e.g. time or space) because measurement across   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> all conditions is too costly.                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> They experimented in four RL environments paired with nine misspecified proxy rewards. The overall findings     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> from these experiments can be summarized as follows: A model of higher capability tends to obtain higher (or    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> similar) proxy rewards but decreased true rewards.                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Model size: Larger model size leads to increased proxy rewards but decreased true rewards.                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Action space resolution: Increased precision in actions leads to more capable agents. However, higher           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> resolution causes proxy rewards to remain constant while true rewards decrease.                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Observation fidelity: More accurate observations improve proxy rewards but slightly reduce true rewards.        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Training steps: Optimizing the proxy reward over more steps harms true rewards after an initial period where    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the rewards are positively correlated.                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The plot of proxy and true reward value as functions of (Top row) model sizes, measured in parameter count;     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (Bottom row) model capability, measured by metrics such as training steps, action space resolution, and         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> observation noise. (Image source: Pan et al. 2022)                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> If a proxy reward is so poorly specified that it has a very weak correlation with the true reward, we may be    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> able to identify and prevent reward hacking even before training. Based on this hypothesis, Pan et al. (2022)   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> investigated the correlation between proxy and true rewards over a collection of trajectory rollouts.           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Interestingly, reward hacking still occurs even when there is a positive correlation between the true and proxy <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> rewards.                                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Hacking RLHF of LLMs#                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reinforcement learning from human feedback (RLHF) has become the de facto approach for alignment training of    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> language models. A reward model is trained on human feedback data and then a language model is fine-tuned via   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RL to optimize this proxy reward for human preference. There are three types of reward we care about in an RLHF <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> setup:                                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m╭─\u001b[0m\u001b[33m───────────────────────────────────────────────\u001b[0m\u001b[33m 🔧 Tool Output \u001b[0m\u001b[33m────────────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward hacking examples in real life#                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The recommendation algorithm for social media is intended to provide useful information. However, usefulness is \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m often measured by proxy metrics, such as the number of likes or comments, or the time or frequency of           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m engagement on the platform. The algorithm ends up recommending content that can affect users’ emotion states    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m such as outrageous and extreme content in order to trigger more engagement. (Harari, 2024)                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Optimizing for misspecified proxy metrics for a video sharing site may aggressively increase the watch time of  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m users while the true goal is to optimize users’ subjective well-being. (Link)                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m “The Big Short” - 2008 financial crisis caused by the housing bubble. Reward hacking of our society happened as \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m people tried to game the financial system.                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Why does Reward Hacking Exist?#                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Goodhart’s Law states that “When a measure becomes a target, it ceases to be a good measure”. The intuition is  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m that a good metric can become corrupted once significant pressure is applied to optimize it. It is challenging  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m to specify a 100% accurate reward objective and any proxy suffers the risk of being hacked, as RL algorithm     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m exploits any small imperfection in the reward function definition. Garrabrant (2017) categorized Goodhart’s law \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m into 4 variants:                                                                                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Regressional - selection for an imperfect proxy necessarily also selects for noise.                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Extremal - the metric selection pushes the state distribution into a region of different data distribution.     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Causal -  when there is a non-causal correlation between the proxy and the goal, intervening on the proxy may   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m fail to intervene on the goal.                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Adversarial - optimization for a proxy provides an incentive for adversaries to correlate their goal with the   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m proxy.                                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Amodei et al. (2016) summarized that reward hacking, mainly in RL setting, may occur due to:                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Partial observed states and goals are imperfect representation of the environment status.                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The system itself is complex and susceptible to hacking; e.g., if the agent is allowed to execute code that     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m changes part of the environment, it becomes much easier to exploit the environment’s mechanisms.                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The reward may involve abstract concept that is hard to be learned or formulated; e.g., a reward function with  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m high-dimensional inputs may disproportionately rely on a few dimensions.                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RL targets to get the reward function highly optimized, so there exists an intrinsic “conflict”, making the     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m design of good RL objective challenging. A special case is a type of the reward function with a                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m self-reinforcing feedback component, where the reward may get amplified and distorted to a point that breaks    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m down the original intent, such as an ads placement algorithm leading to winners getting all.                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Besides, identifying the exact reward function for which an optimal agent optimizes its behavior is in general  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m impossible since there could be an infinite number of reward functions consistent with any observed policy in   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m an fixed environment (Ng & Russell, 2000). Amin and Singh (2016) separated the causes of this unidentifiability \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m into two classes:                                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Representational - a set of reward functions is behaviorally invariant under certain arithmetic operations      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (e.g., re-scaling)                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Experimental - $\\pi$’s observed behavior is insufficient to distinguish between two or more reward functions    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m which both rationalize the behavior of the agent (the behavior is optimal under both)                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward Hacking in Reinforcement Learning | Lil'Log                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Lil'Log                                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m |                                                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Posts                                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Archive                                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Search                                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Tags                                                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m FAQ                                                                                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m       Reward Hacking in Reinforcement Learning                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Date: November 28, 2024  |  Estimated Reading Time: 37 min  |  Author: Lilian Weng                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Table of Contents                                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Background                                                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward Function in RL                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Spurious Correlation                                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Let’s Define Reward Hacking                                                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m List of Examples                                                                                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward hacking examples in RL tasks                                                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward hacking examples in LLM tasks                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward hacking examples in real life                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Why does Reward Hacking Exist?                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Hacking RL Environment                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Hacking RLHF of LLMs                                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Hacking the Training Process                                                                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Hacking the Evaluator                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m In-Context Reward Hacking                                                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Generalization of Hacking Skills                                                                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Peek into Mitigations                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RL Algorithm Improvement                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Detecting Reward Hacking                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Data Analysis of RLHF                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Citation                                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m References                                                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward hacking occurs when a reinforcement learning (RL) agent exploits flaws or ambiguities in the reward      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m function to achieve high rewards, without genuinely learning or completing the intended task. Reward hacking    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m exists because RL environments are often imperfect, and it is fundamentally challenging to accurately specify a \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m reward function.                                                                                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m With the rise of language models generalizing to a broad spectrum of tasks and RLHF becomes a de facto method   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m for alignment training, reward hacking in RL training of language models has become a critical practical        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m challenge. Instances where the model learns to modify unit tests to pass coding tasks, or where responses       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m contain biases that mimic a user’s preference, are pretty concerning and are likely one of the major blockers   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m for real-world deployment of more autonomous use cases of AI models.                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Most of the past work on this topic has been quite theoretical and focused on defining or demonstrating the     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m existence of reward hacking. However, research into practical mitigations, especially in the context of RLHF    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m and LLMs, remains limited. I especially want to call out for more research efforts directed toward              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m understanding and developing mitigation for reward hacking in the future. Hope I will be able to cover the      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m mitigation part in a dedicated post soon.                                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Background#                                                                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward Function in RL#                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward function defines the task, and reward shaping significantly impacts learning efficiency and accuracy in  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m reinforcement learning. Designing a reward function for an RL task often feels like a ‘dark art’. Many factors  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m contribute to this complexity: How you decompose a big goal into small goals? Is the reward sparse or dense?    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m How you measure the success? Various choices may lead to good or problematic learning dynamics, including       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m unlearnable tasks or hackable reward functions. There is a long history of research on how to do reward shaping \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m in RL.                                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m For example, in an 1999 paper by Ng et al., the authors studied how to modify the reward function in Markov     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Decision Processes (MDPs) such that the optimal policy remains unchanged. They found that linear transformation \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m works. Given a MDP $M = (S, A, T, \\gamma, R)$, we want to create a transformed MDP $M’ = (S, A, T, \\gamma, R’)$ \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m where $R’ = R + F$ and $F: S \\times A \\times S \\mapsto \\mathbb{R}$, such that we can guide the learning         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m algorithm to be more efficient. Given a real-valued function $\\Phi: S \\mapsto \\mathbb{R}$, $F$ is a             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m potential-based shaping function if for all $s \\in S - {s_0}, a \\in A, s’ \\in S$:                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m F(s, a, s') = \\gamma \\Phi(s') - \\Phi(s)                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Quantitative generalization results of a model trained with expert iteration according to our curriculum from   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m each stage to the next. (Image source: Denison et al. 2024)                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m It is noteworthy that even after the curriculum, the model overwrote the reward and avoided detection less than \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m 1/1000 of the time. Even when a model was trained on curricula which directly incentivized reward hacking, the  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m model overwrote their reward less than 1% of the time and hacked unit tests even less often. As a simple        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m mitigation, supervised fine-tuning the model on the first two environments–where the reward hacking behavior is \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m easy to be detected (sycophancy and flattery)—with SFT data that does not game the env was found to reduce the  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m likelihood of reward tampering in holdout environments.                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Peek into Mitigations#                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m While there is a large body of literature discussing the phenomenon of reward hacking, there has been not a ton \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m of work on mitigations for reward hacking, especially in the area of RLHF and LLMs. Let’s lightly review three  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m potential approaches in this section, not exhaustive yet.                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RL Algorithm Improvement#                                                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Amodei et al. (2016) pointed out some directions for mitigating reward hacking in RL training:                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Adversarial reward functions. We treat the reward function as an adaptive agent itself and it can adapt to new  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m tricks that the model discovered where the reward is high but human rating is low.                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Model lookahead. It is possible to give reward based on future anticipated states; e.g., if the agent is gonna  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m replace the reward function, it gets negative rewards.                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Adversarial blinding. We can blind the model with certain variables such that the agent cannot learn            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m information that enables it to hack the reward function.                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Careful engineering. Some types of reward hacking against the system design can be avoided by careful           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m engineering; e.g., sandboxing the agent to isolate its actions from its reward signals.                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward capping. This strategy is to simply limit the maximum possible reward, as it can effectively prevent     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m rare events of the agent hacking to get a super high pay-off strategy.                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Counterexample resistance. Improvement on adversarial robustness should benefit the robustness of the reward    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m function.                                                                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Combination of multiple rewards. Combining different types of rewards could make it harder to be hacked.        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward pretraining. We can learn a reward function from a collection of (state, reward) samples, but depending  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m on how well this supervised training setup is, it may come with other baggages. RLHF depends on this but        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m learned scalar reward models are quite vulnerable to learning undesired traits.                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Variable indifference. The goal is to ask the agent to optimize some variables in the environment but not       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m others.                                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Trip wires. We can intentionally introduce some vulnerabilities and set up monitoring and alerts if any gets    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m reward hacked.                                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m In RL setups where human feedback is formed as approval of agent actions, Uesato et al. (2020) proposed to      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m prevent reward tampering with decoupled approval.  If the feedback is conditioned on $(s, a)$ (state, action),  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m we can never get uncorrupted feedback for action $a$ at state $s$ once reward tampering happens for this pair.  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Decoupling means that the query action for collecting feedback is sampled independently from the action taken   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m in the world. Feedback is received even before the action is executed in the world, thus preventing the action  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m from corrupting its own feedback.                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Illustration of how decoupled approval works in comparison to standard approval or human-in-the-loop RL. (Image \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m source: Uesato et al. 2020)                                                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m With decoupled approval, the action (taken in the world) and the query (for getting user approval feedback) are \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m sampled independently. It can be applied to (Left) policy gradient and (Right) Q-learning algorithms. (Image    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m source: Uesato et al. 2020)                                                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Detecting Reward Hacking#                                                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m An alternative mitigation is to detect reward hacking by framing it as an anomaly detection task, where the     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m detector (“a trusted policy” with trajectories and rewards validated by human) should flag instances of         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m misalignment (Pan et al. 2022). Given (1) a trusted policy and (2) a collection of manually labeled trajectory  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m rollouts, we can build a binary classifier based on distances between action distribution of two policies, the  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m trusted policy and the target policy, and measure the accuracy of this anomaly detection classifier. In         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m experiments by Pan et al. (2022), they observed that different detectors are better for different tasks and     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m none of the tested classifier can achieve AUROC greater than 60% across all tested RL environments.             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Performance of detectors on different tasks. (Image source: Pan et al. 2022)                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Hacking RL Environment#                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward hacking is expected to be a more common problem as the model and the algorithm become increasingly       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m sophisticated. A more intelligent agent is more capable of finding “holes” in the design of reward function and \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m exploiting the task specification—in other words, achieving higher proxy rewards but lower true rewards. By     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m contrast, a weaker algorithm may not be able to find such loopholes, and thus we would not observe any reward   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m hacking or identify issues in the current reward function design when the model is not strong enough.           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m In a set of zero-sum robotics self-play games (Bansal et al., 2017), we can train two agents (victim vs.        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m opponent) to compete against each other. A standard training process produces a victim agent with adequate      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m performance when playing against a normal opponent. However, it is easy to train an adversarial opponent policy \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m that can defeat the victim reliably despite outputting seemingly random actions and training with fewer than 3% \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m of time steps (Gleave et al., 2020). Training of adversarial policies involves optimizing the sum of discounted \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m rewards, as in standard RL setup, while treating the victim policy as a black-box model.                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m An intuitive way to mitigate adversarial policies attacks is to fine-tune victims against adversarial policies. \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m However, the victim remains vulnerable to new versions of adversarial policies once retrained against the new   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m victim policy.                                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Why does adversarial policy exist? The hypothesis is that adversarial policies introduce OOD observations to    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the victim rather than physically interfering with it. Evidence shows that when the victim’s observation of the \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m opponent’s position is masked and set to a static state, the victim becomes more robust to adversaries,         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m although performing worse against a normal opponent policy. Furthermore, a higher-dimensional observation space \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m enhances performance under normal circumstances but makes the policy more vulnerable to adversarial opponents.  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Pan et al. (2022) investigated reward hacking as a function of agent capabilities, including (1) model size,    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (2) action space resolution, (3) observation space noise, and (4) training time. They also proposed a taxonomy  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m of three types of misspecified proxy rewards:                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Misweighting: Proxy and true rewards capture the same desiderata, but differ in their relative importance.      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Ontological: Proxy and true rewards use different desiderata to capture the same concept.                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Scope: The proxy measures desiderata over a restricted domain (e.g. time or space) because measurement across   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m all conditions is too costly.                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m They experimented in four RL environments paired with nine misspecified proxy rewards. The overall findings     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m from these experiments can be summarized as follows: A model of higher capability tends to obtain higher (or    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m similar) proxy rewards but decreased true rewards.                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Model size: Larger model size leads to increased proxy rewards but decreased true rewards.                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Action space resolution: Increased precision in actions leads to more capable agents. However, higher           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m resolution causes proxy rewards to remain constant while true rewards decrease.                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Observation fidelity: More accurate observations improve proxy rewards but slightly reduce true rewards.        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Training steps: Optimizing the proxy reward over more steps harms true rewards after an initial period where    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the rewards are positively correlated.                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The plot of proxy and true reward value as functions of (Top row) model sizes, measured in parameter count;     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (Bottom row) model capability, measured by metrics such as training steps, action space resolution, and         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m observation noise. (Image source: Pan et al. 2022)                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m If a proxy reward is so poorly specified that it has a very weak correlation with the true reward, we may be    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m able to identify and prevent reward hacking even before training. Based on this hypothesis, Pan et al. (2022)   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m investigated the correlation between proxy and true rewards over a collection of trajectory rollouts.           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Interestingly, reward hacking still occurs even when there is a positive correlation between the true and proxy \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m rewards.                                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Hacking RLHF of LLMs#                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reinforcement learning from human feedback (RLHF) has become the de facto approach for alignment training of    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m language models. A reward model is trained on human feedback data and then a language model is fine-tuned via   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RL to optimize this proxy reward for human preference. There are three types of reward we care about in an RLHF \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m setup:                                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╭───────────────────────────────────────────────────── 📝 AI ─────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> Let me search for more specific information about the types of reward hacking discussed in the blogs.           <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37m╭─\u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m 📝 AI \u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m─╮\u001b[0m\n",
       "\u001b[37m│\u001b[0m Let me search for more specific information about the types of reward hacking discussed in the blogs.           \u001b[37m│\u001b[0m\n",
       "\u001b[37m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">╭──────────────────────────────────────────────── 🔧 Tool Output ─────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> F(s, a, s') = \\gamma \\Phi(s') - \\Phi(s)                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> This would guarantee that the sum of discounted $F$, $F(s_1, a_1, s_2) + \\gamma F(s_2, a_2, s_3) + \\dots$, ends <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> up being 0. If $F$ is such a potential-based shaping function, it is both sufficient and necessary to ensure    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $M$ and $M’$ share the same optimal policies.                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> When $F(s, a, s’) = \\gamma \\Phi(s’) - \\Phi(s)$, and if we further assume that $\\Phi(s_0) = 0$, where $s_0$ is   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> absorbing state, and $\\gamma=1$, and then for all $s \\in S, a \\in A$:                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\begin{aligned}                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Q^*_{M'} (s,a) &amp;= Q^*_M(s, a) - \\Phi(s) \\\\                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> V^*_{M'} (s,a) &amp;= V^*_M(s, a) - \\Phi(s)                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\end{aligned}                                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> This form of reward shaping allows us to incorporate heuristics into the reward function to speed up learning   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> without impacting the optimal policy.                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Spurious Correlation#                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Spurious correlation or shortcut learning (Geirhos et al. 2020) in classification task is a concept closely     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> related to reward hacking. Spurious or shortcut features can cause a classifier to fail at learning and         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> generalizing as intended. For example, a binary classifier for distinguishing wolves from huskies may overfit   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> to the presence of a snowy background if all the wolf training images include snow (Ribeiro et al. 2024).       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The model performs poorly on out-of-distribution (OOD) test sets if it overfits to shortcut features. (Image    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> source: Geirhos et al. 2020)                                                                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The ERM principle states that, since the full data distribution is unknown, minimizing the loss on training     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> data is a reasonable proxy of risk and thus we favor models with the lowest training loss. Nagarajan et al.     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (2021) studied the ERM principle and pointed out that ERM needs to rely on all types of informative features,   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> including unreliable spurious features, while attempting to fit the data without constraints. Their experiments <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> showed that ERM would depend on spurious features no matter how easy the task is.                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Let’s Define Reward Hacking#                                                                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward shaping in RL is challenging. Reward hacking occurs when an RL agent exploits flaws or ambiguities in    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the reward function to obtain high rewards without genuinely learning the intended behaviors or completing the  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> task as designed. In recent years, several related concepts have been proposed, all referring to some form of   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> reward hacking:                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward hacking (Amodei et al., 2016)                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward corruption (Everitt et al., 2017)                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward tampering (Everitt et al. 2019)                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Specification gaming (Krakovna et al., 2020)                                                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Objective robustness (Koch et al. 2021)                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Goal misgeneralization (Langosco et al. 2022)                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward misspecifications (Pan et al. 2022)                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The concept originated with Amodei et al. (2016), who proposed a set of open research questions on AI safety in <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> their seminal paper “Concrete Problems in AI Safety”. They listed reward hacking as one of the key AI safety    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> problems. Reward hacking refers to the possibility of the agent gaming the reward function to achieve high      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> reward through undesired behavior.  Specification gaming (Krakovna et al. 2020) is a similar concept, defined   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> as a behavior that satisfies the literal specification of an objective but not achieving the desired results.   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Here the literal description of the task goal and the intended goal may have a gap.                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward shaping is a technique used to enrich the reward function, making it easier for the agent to learn—for   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> example, by providing denser rewards. However, a poorly design reward shaping mechanism can alter the           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> trajectory of the optimal policy. Designing effective reward shaping mechanisms is inherently difficult. Rather <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> than blaming a poorly designed reward function, it is more accurate to acknowledge that designing a good reward <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> function is intrinsically challenging due to the complexity of the task itself, partial observable state,       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> multiple dimensions in consideration, and other factors.                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> When testing an RL agent in out-of-distribution (OOD) environments, robustness failure may occur due to:        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Hacking RL Environment#                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward hacking is expected to be a more common problem as the model and the algorithm become increasingly       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> sophisticated. A more intelligent agent is more capable of finding “holes” in the design of reward function and <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> exploiting the task specification—in other words, achieving higher proxy rewards but lower true rewards. By     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> contrast, a weaker algorithm may not be able to find such loopholes, and thus we would not observe any reward   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> hacking or identify issues in the current reward function design when the model is not strong enough.           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> In a set of zero-sum robotics self-play games (Bansal et al., 2017), we can train two agents (victim vs.        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> opponent) to compete against each other. A standard training process produces a victim agent with adequate      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> performance when playing against a normal opponent. However, it is easy to train an adversarial opponent policy <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> that can defeat the victim reliably despite outputting seemingly random actions and training with fewer than 3% <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> of time steps (Gleave et al., 2020). Training of adversarial policies involves optimizing the sum of discounted <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> rewards, as in standard RL setup, while treating the victim policy as a black-box model.                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> An intuitive way to mitigate adversarial policies attacks is to fine-tune victims against adversarial policies. <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> However, the victim remains vulnerable to new versions of adversarial policies once retrained against the new   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> victim policy.                                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Why does adversarial policy exist? The hypothesis is that adversarial policies introduce OOD observations to    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the victim rather than physically interfering with it. Evidence shows that when the victim’s observation of the <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> opponent’s position is masked and set to a static state, the victim becomes more robust to adversaries,         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> although performing worse against a normal opponent policy. Furthermore, a higher-dimensional observation space <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> enhances performance under normal circumstances but makes the policy more vulnerable to adversarial opponents.  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Pan et al. (2022) investigated reward hacking as a function of agent capabilities, including (1) model size,    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (2) action space resolution, (3) observation space noise, and (4) training time. They also proposed a taxonomy  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> of three types of misspecified proxy rewards:                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Misweighting: Proxy and true rewards capture the same desiderata, but differ in their relative importance.      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Ontological: Proxy and true rewards use different desiderata to capture the same concept.                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Scope: The proxy measures desiderata over a restricted domain (e.g. time or space) because measurement across   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> all conditions is too costly.                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> They experimented in four RL environments paired with nine misspecified proxy rewards. The overall findings     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> from these experiments can be summarized as follows: A model of higher capability tends to obtain higher (or    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> similar) proxy rewards but decreased true rewards.                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Model size: Larger model size leads to increased proxy rewards but decreased true rewards.                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Action space resolution: Increased precision in actions leads to more capable agents. However, higher           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> resolution causes proxy rewards to remain constant while true rewards decrease.                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Observation fidelity: More accurate observations improve proxy rewards but slightly reduce true rewards.        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Training steps: Optimizing the proxy reward over more steps harms true rewards after an initial period where    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the rewards are positively correlated.                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The plot of proxy and true reward value as functions of (Top row) model sizes, measured in parameter count;     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (Bottom row) model capability, measured by metrics such as training steps, action space resolution, and         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> observation noise. (Image source: Pan et al. 2022)                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> If a proxy reward is so poorly specified that it has a very weak correlation with the true reward, we may be    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> able to identify and prevent reward hacking even before training. Based on this hypothesis, Pan et al. (2022)   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> investigated the correlation between proxy and true rewards over a collection of trajectory rollouts.           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Interestingly, reward hacking still occurs even when there is a positive correlation between the true and proxy <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> rewards.                                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Hacking RLHF of LLMs#                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reinforcement learning from human feedback (RLHF) has become the de facto approach for alignment training of    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> language models. A reward model is trained on human feedback data and then a language model is fine-tuned via   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RL to optimize this proxy reward for human preference. There are three types of reward we care about in an RLHF <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> setup:                                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Then we can infer,                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\begin{aligned}                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\sin\\phi\\mathbf{x}                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> &amp;= \\cos\\phi\\boldsymbol{\\epsilon}  - \\mathbf{v}_\\phi \\\\                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> &amp;= \\frac{\\cos\\phi}{\\sin\\phi}\\big(\\mathbf{z}_\\phi - \\cos\\phi\\mathbf{x}\\big) - \\mathbf{v}_\\phi \\\\                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\sin^2\\phi\\mathbf{x}                                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> &amp;= \\cos\\phi\\mathbf{z}_\\phi - \\cos^2\\phi\\mathbf{x} - \\sin\\phi \\mathbf{v}_\\phi \\\\                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\mathbf{x} &amp;= \\cos\\phi\\mathbf{z}_\\phi - \\sin\\phi\\mathbf{v}_\\phi \\\\                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\text{Similarly }                                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\boldsymbol{\\epsilon} &amp;= \\sin\\phi\\mathbf{z}_\\phi + \\cos\\phi \\mathbf{v}_\\phi                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\end{aligned}                                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The DDIM update rule is updated accordingly,                                                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\begin{aligned}                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\mathbf{z}_{\\phi_s}                                                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> &amp;= \\cos\\phi_s\\hat{\\mathbf{x}}_\\theta(\\mathbf{z}_{\\phi_t}) +                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\sin\\phi_s\\hat{\\epsilon}_\\theta(\\mathbf{z}_{\\phi_t}) \\quad\\quad{\\small \\text{; }\\hat{\\mathbf{x}}_\\theta(.),     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\hat{\\epsilon}_\\theta(.)\\text{ are two models to predict }\\mathbf{x}, \\boldsymbol{\\epsilon}\\text{ based on      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> }\\mathbf{z}_{\\phi_t}}\\\\                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> &amp;= \\cos\\phi_s \\big( \\cos\\phi_t \\mathbf{z}_{\\phi_t} - \\sin\\phi_t \\hat{\\mathbf{v}}_\\theta(\\mathbf{z}_{\\phi_t} )   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\big) +                                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\sin\\phi_s \\big( \\sin\\phi_t \\mathbf{z}_{\\phi_t} + \\cos\\phi_t \\hat{\\mathbf{v}}_\\theta(\\mathbf{z}_{\\phi_t} )      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\big) \\\\                                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> &amp;= {\\color{red} \\big( \\cos\\phi_s\\cos\\phi_t + \\sin\\phi_s\\sin\\phi_t \\big)} \\mathbf{z}_{\\phi_t} +                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> {\\color{green} \\big( \\sin\\phi_s \\cos\\phi_t - \\cos\\phi_s \\sin\\phi_t \\big)}                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\hat{\\mathbf{v}}_\\theta(\\mathbf{z}_{\\phi_t} ) \\\\                                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> &amp;= {\\color{red} cos(\\phi_s - \\phi_t)} \\mathbf{z}_{\\phi_t} +                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> {\\color{green} \\sin(\\phi_s - \\phi_t)} \\hat{\\mathbf{v}}_\\theta(\\mathbf{z}_{\\phi_t}) \\quad\\quad{\\small \\text{;    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> trigonometric identity functions.}}                                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\end{aligned}                                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Visualizing how the diffusion update step works in the angular coordinate, where DDIM evolves                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $\\mathbf{z}_{\\phi_s}$ by moving it along the $-\\hat{\\mathbf{v}}_{\\phi_t}$ direction. (Image source: Salimans &amp;  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Ho, 2022)                                                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward hacking examples in real life#                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The recommendation algorithm for social media is intended to provide useful information. However, usefulness is <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> often measured by proxy metrics, such as the number of likes or comments, or the time or frequency of           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> engagement on the platform. The algorithm ends up recommending content that can affect users’ emotion states    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> such as outrageous and extreme content in order to trigger more engagement. (Harari, 2024)                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Optimizing for misspecified proxy metrics for a video sharing site may aggressively increase the watch time of  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> users while the true goal is to optimize users’ subjective well-being. (Link)                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> “The Big Short” - 2008 financial crisis caused by the housing bubble. Reward hacking of our society happened as <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> people tried to game the financial system.                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Why does Reward Hacking Exist?#                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Goodhart’s Law states that “When a measure becomes a target, it ceases to be a good measure”. The intuition is  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> that a good metric can become corrupted once significant pressure is applied to optimize it. It is challenging  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> to specify a 100% accurate reward objective and any proxy suffers the risk of being hacked, as RL algorithm     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> exploits any small imperfection in the reward function definition. Garrabrant (2017) categorized Goodhart’s law <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> into 4 variants:                                                                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Regressional - selection for an imperfect proxy necessarily also selects for noise.                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Extremal - the metric selection pushes the state distribution into a region of different data distribution.     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Causal -  when there is a non-causal correlation between the proxy and the goal, intervening on the proxy may   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> fail to intervene on the goal.                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Adversarial - optimization for a proxy provides an incentive for adversaries to correlate their goal with the   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> proxy.                                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Amodei et al. (2016) summarized that reward hacking, mainly in RL setting, may occur due to:                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Partial observed states and goals are imperfect representation of the environment status.                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The system itself is complex and susceptible to hacking; e.g., if the agent is allowed to execute code that     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> changes part of the environment, it becomes much easier to exploit the environment’s mechanisms.                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The reward may involve abstract concept that is hard to be learned or formulated; e.g., a reward function with  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> high-dimensional inputs may disproportionately rely on a few dimensions.                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RL targets to get the reward function highly optimized, so there exists an intrinsic “conflict”, making the     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> design of good RL objective challenging. A special case is a type of the reward function with a                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> self-reinforcing feedback component, where the reward may get amplified and distorted to a point that breaks    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> down the original intent, such as an ads placement algorithm leading to winners getting all.                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Besides, identifying the exact reward function for which an optimal agent optimizes its behavior is in general  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> impossible since there could be an infinite number of reward functions consistent with any observed policy in   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> an fixed environment (Ng &amp; Russell, 2000). Amin and Singh (2016) separated the causes of this unidentifiability <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> into two classes:                                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Representational - a set of reward functions is behaviorally invariant under certain arithmetic operations      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (e.g., re-scaling)                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Experimental - $\\pi$’s observed behavior is insufficient to distinguish between two or more reward functions    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> which both rationalize the behavior of the agent (the behavior is optimal under both)                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m╭─\u001b[0m\u001b[33m───────────────────────────────────────────────\u001b[0m\u001b[33m 🔧 Tool Output \u001b[0m\u001b[33m────────────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m F(s, a, s') = \\gamma \\Phi(s') - \\Phi(s)                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m This would guarantee that the sum of discounted $F$, $F(s_1, a_1, s_2) + \\gamma F(s_2, a_2, s_3) + \\dots$, ends \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m up being 0. If $F$ is such a potential-based shaping function, it is both sufficient and necessary to ensure    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $M$ and $M’$ share the same optimal policies.                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m When $F(s, a, s’) = \\gamma \\Phi(s’) - \\Phi(s)$, and if we further assume that $\\Phi(s_0) = 0$, where $s_0$ is   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m absorbing state, and $\\gamma=1$, and then for all $s \\in S, a \\in A$:                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\begin{aligned}                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Q^*_{M'} (s,a) &= Q^*_M(s, a) - \\Phi(s) \\\\                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m V^*_{M'} (s,a) &= V^*_M(s, a) - \\Phi(s)                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\end{aligned}                                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m This form of reward shaping allows us to incorporate heuristics into the reward function to speed up learning   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m without impacting the optimal policy.                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Spurious Correlation#                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Spurious correlation or shortcut learning (Geirhos et al. 2020) in classification task is a concept closely     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m related to reward hacking. Spurious or shortcut features can cause a classifier to fail at learning and         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m generalizing as intended. For example, a binary classifier for distinguishing wolves from huskies may overfit   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m to the presence of a snowy background if all the wolf training images include snow (Ribeiro et al. 2024).       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The model performs poorly on out-of-distribution (OOD) test sets if it overfits to shortcut features. (Image    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m source: Geirhos et al. 2020)                                                                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The ERM principle states that, since the full data distribution is unknown, minimizing the loss on training     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m data is a reasonable proxy of risk and thus we favor models with the lowest training loss. Nagarajan et al.     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (2021) studied the ERM principle and pointed out that ERM needs to rely on all types of informative features,   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m including unreliable spurious features, while attempting to fit the data without constraints. Their experiments \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m showed that ERM would depend on spurious features no matter how easy the task is.                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Let’s Define Reward Hacking#                                                                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward shaping in RL is challenging. Reward hacking occurs when an RL agent exploits flaws or ambiguities in    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the reward function to obtain high rewards without genuinely learning the intended behaviors or completing the  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m task as designed. In recent years, several related concepts have been proposed, all referring to some form of   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m reward hacking:                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward hacking (Amodei et al., 2016)                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward corruption (Everitt et al., 2017)                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward tampering (Everitt et al. 2019)                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Specification gaming (Krakovna et al., 2020)                                                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Objective robustness (Koch et al. 2021)                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Goal misgeneralization (Langosco et al. 2022)                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward misspecifications (Pan et al. 2022)                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The concept originated with Amodei et al. (2016), who proposed a set of open research questions on AI safety in \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m their seminal paper “Concrete Problems in AI Safety”. They listed reward hacking as one of the key AI safety    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m problems. Reward hacking refers to the possibility of the agent gaming the reward function to achieve high      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m reward through undesired behavior.  Specification gaming (Krakovna et al. 2020) is a similar concept, defined   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m as a behavior that satisfies the literal specification of an objective but not achieving the desired results.   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Here the literal description of the task goal and the intended goal may have a gap.                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward shaping is a technique used to enrich the reward function, making it easier for the agent to learn—for   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m example, by providing denser rewards. However, a poorly design reward shaping mechanism can alter the           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m trajectory of the optimal policy. Designing effective reward shaping mechanisms is inherently difficult. Rather \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m than blaming a poorly designed reward function, it is more accurate to acknowledge that designing a good reward \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m function is intrinsically challenging due to the complexity of the task itself, partial observable state,       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m multiple dimensions in consideration, and other factors.                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m When testing an RL agent in out-of-distribution (OOD) environments, robustness failure may occur due to:        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Hacking RL Environment#                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward hacking is expected to be a more common problem as the model and the algorithm become increasingly       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m sophisticated. A more intelligent agent is more capable of finding “holes” in the design of reward function and \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m exploiting the task specification—in other words, achieving higher proxy rewards but lower true rewards. By     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m contrast, a weaker algorithm may not be able to find such loopholes, and thus we would not observe any reward   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m hacking or identify issues in the current reward function design when the model is not strong enough.           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m In a set of zero-sum robotics self-play games (Bansal et al., 2017), we can train two agents (victim vs.        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m opponent) to compete against each other. A standard training process produces a victim agent with adequate      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m performance when playing against a normal opponent. However, it is easy to train an adversarial opponent policy \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m that can defeat the victim reliably despite outputting seemingly random actions and training with fewer than 3% \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m of time steps (Gleave et al., 2020). Training of adversarial policies involves optimizing the sum of discounted \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m rewards, as in standard RL setup, while treating the victim policy as a black-box model.                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m An intuitive way to mitigate adversarial policies attacks is to fine-tune victims against adversarial policies. \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m However, the victim remains vulnerable to new versions of adversarial policies once retrained against the new   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m victim policy.                                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Why does adversarial policy exist? The hypothesis is that adversarial policies introduce OOD observations to    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the victim rather than physically interfering with it. Evidence shows that when the victim’s observation of the \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m opponent’s position is masked and set to a static state, the victim becomes more robust to adversaries,         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m although performing worse against a normal opponent policy. Furthermore, a higher-dimensional observation space \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m enhances performance under normal circumstances but makes the policy more vulnerable to adversarial opponents.  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Pan et al. (2022) investigated reward hacking as a function of agent capabilities, including (1) model size,    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (2) action space resolution, (3) observation space noise, and (4) training time. They also proposed a taxonomy  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m of three types of misspecified proxy rewards:                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Misweighting: Proxy and true rewards capture the same desiderata, but differ in their relative importance.      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Ontological: Proxy and true rewards use different desiderata to capture the same concept.                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Scope: The proxy measures desiderata over a restricted domain (e.g. time or space) because measurement across   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m all conditions is too costly.                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m They experimented in four RL environments paired with nine misspecified proxy rewards. The overall findings     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m from these experiments can be summarized as follows: A model of higher capability tends to obtain higher (or    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m similar) proxy rewards but decreased true rewards.                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Model size: Larger model size leads to increased proxy rewards but decreased true rewards.                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Action space resolution: Increased precision in actions leads to more capable agents. However, higher           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m resolution causes proxy rewards to remain constant while true rewards decrease.                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Observation fidelity: More accurate observations improve proxy rewards but slightly reduce true rewards.        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Training steps: Optimizing the proxy reward over more steps harms true rewards after an initial period where    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the rewards are positively correlated.                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The plot of proxy and true reward value as functions of (Top row) model sizes, measured in parameter count;     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (Bottom row) model capability, measured by metrics such as training steps, action space resolution, and         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m observation noise. (Image source: Pan et al. 2022)                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m If a proxy reward is so poorly specified that it has a very weak correlation with the true reward, we may be    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m able to identify and prevent reward hacking even before training. Based on this hypothesis, Pan et al. (2022)   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m investigated the correlation between proxy and true rewards over a collection of trajectory rollouts.           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Interestingly, reward hacking still occurs even when there is a positive correlation between the true and proxy \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m rewards.                                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Hacking RLHF of LLMs#                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reinforcement learning from human feedback (RLHF) has become the de facto approach for alignment training of    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m language models. A reward model is trained on human feedback data and then a language model is fine-tuned via   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RL to optimize this proxy reward for human preference. There are three types of reward we care about in an RLHF \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m setup:                                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Then we can infer,                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\begin{aligned}                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\sin\\phi\\mathbf{x}                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m &= \\cos\\phi\\boldsymbol{\\epsilon}  - \\mathbf{v}_\\phi \\\\                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m &= \\frac{\\cos\\phi}{\\sin\\phi}\\big(\\mathbf{z}_\\phi - \\cos\\phi\\mathbf{x}\\big) - \\mathbf{v}_\\phi \\\\                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\sin^2\\phi\\mathbf{x}                                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m &= \\cos\\phi\\mathbf{z}_\\phi - \\cos^2\\phi\\mathbf{x} - \\sin\\phi \\mathbf{v}_\\phi \\\\                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\mathbf{x} &= \\cos\\phi\\mathbf{z}_\\phi - \\sin\\phi\\mathbf{v}_\\phi \\\\                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\text{Similarly }                                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\boldsymbol{\\epsilon} &= \\sin\\phi\\mathbf{z}_\\phi + \\cos\\phi \\mathbf{v}_\\phi                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\end{aligned}                                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The DDIM update rule is updated accordingly,                                                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\begin{aligned}                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\mathbf{z}_{\\phi_s}                                                                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m &= \\cos\\phi_s\\hat{\\mathbf{x}}_\\theta(\\mathbf{z}_{\\phi_t}) +                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\sin\\phi_s\\hat{\\epsilon}_\\theta(\\mathbf{z}_{\\phi_t}) \\quad\\quad{\\small \\text{; }\\hat{\\mathbf{x}}_\\theta(.),     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\hat{\\epsilon}_\\theta(.)\\text{ are two models to predict }\\mathbf{x}, \\boldsymbol{\\epsilon}\\text{ based on      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m }\\mathbf{z}_{\\phi_t}}\\\\                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m &= \\cos\\phi_s \\big( \\cos\\phi_t \\mathbf{z}_{\\phi_t} - \\sin\\phi_t \\hat{\\mathbf{v}}_\\theta(\\mathbf{z}_{\\phi_t} )   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\big) +                                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\sin\\phi_s \\big( \\sin\\phi_t \\mathbf{z}_{\\phi_t} + \\cos\\phi_t \\hat{\\mathbf{v}}_\\theta(\\mathbf{z}_{\\phi_t} )      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\big) \\\\                                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m &= {\\color{red} \\big( \\cos\\phi_s\\cos\\phi_t + \\sin\\phi_s\\sin\\phi_t \\big)} \\mathbf{z}_{\\phi_t} +                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m {\\color{green} \\big( \\sin\\phi_s \\cos\\phi_t - \\cos\\phi_s \\sin\\phi_t \\big)}                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\hat{\\mathbf{v}}_\\theta(\\mathbf{z}_{\\phi_t} ) \\\\                                                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m &= {\\color{red} cos(\\phi_s - \\phi_t)} \\mathbf{z}_{\\phi_t} +                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m {\\color{green} \\sin(\\phi_s - \\phi_t)} \\hat{\\mathbf{v}}_\\theta(\\mathbf{z}_{\\phi_t}) \\quad\\quad{\\small \\text{;    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m trigonometric identity functions.}}                                                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\end{aligned}                                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Visualizing how the diffusion update step works in the angular coordinate, where DDIM evolves                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $\\mathbf{z}_{\\phi_s}$ by moving it along the $-\\hat{\\mathbf{v}}_{\\phi_t}$ direction. (Image source: Salimans &  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Ho, 2022)                                                                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward hacking examples in real life#                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The recommendation algorithm for social media is intended to provide useful information. However, usefulness is \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m often measured by proxy metrics, such as the number of likes or comments, or the time or frequency of           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m engagement on the platform. The algorithm ends up recommending content that can affect users’ emotion states    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m such as outrageous and extreme content in order to trigger more engagement. (Harari, 2024)                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Optimizing for misspecified proxy metrics for a video sharing site may aggressively increase the watch time of  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m users while the true goal is to optimize users’ subjective well-being. (Link)                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m “The Big Short” - 2008 financial crisis caused by the housing bubble. Reward hacking of our society happened as \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m people tried to game the financial system.                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Why does Reward Hacking Exist?#                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Goodhart’s Law states that “When a measure becomes a target, it ceases to be a good measure”. The intuition is  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m that a good metric can become corrupted once significant pressure is applied to optimize it. It is challenging  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m to specify a 100% accurate reward objective and any proxy suffers the risk of being hacked, as RL algorithm     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m exploits any small imperfection in the reward function definition. Garrabrant (2017) categorized Goodhart’s law \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m into 4 variants:                                                                                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Regressional - selection for an imperfect proxy necessarily also selects for noise.                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Extremal - the metric selection pushes the state distribution into a region of different data distribution.     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Causal -  when there is a non-causal correlation between the proxy and the goal, intervening on the proxy may   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m fail to intervene on the goal.                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Adversarial - optimization for a proxy provides an incentive for adversaries to correlate their goal with the   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m proxy.                                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Amodei et al. (2016) summarized that reward hacking, mainly in RL setting, may occur due to:                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Partial observed states and goals are imperfect representation of the environment status.                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The system itself is complex and susceptible to hacking; e.g., if the agent is allowed to execute code that     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m changes part of the environment, it becomes much easier to exploit the environment’s mechanisms.                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The reward may involve abstract concept that is hard to be learned or formulated; e.g., a reward function with  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m high-dimensional inputs may disproportionately rely on a few dimensions.                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RL targets to get the reward function highly optimized, so there exists an intrinsic “conflict”, making the     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m design of good RL objective challenging. A special case is a type of the reward function with a                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m self-reinforcing feedback component, where the reward may get amplified and distorted to a point that breaks    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m down the original intent, such as an ads placement algorithm leading to winners getting all.                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Besides, identifying the exact reward function for which an optimal agent optimizes its behavior is in general  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m impossible since there could be an infinite number of reward functions consistent with any observed policy in   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m an fixed environment (Ng & Russell, 2000). Amin and Singh (2016) separated the causes of this unidentifiability \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m into two classes:                                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Representational - a set of reward functions is behaviorally invariant under certain arithmetic operations      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (e.g., re-scaling)                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Experimental - $\\pi$’s observed behavior is insufficient to distinguish between two or more reward functions    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m which both rationalize the behavior of the agent (the behavior is optimal under both)                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╭───────────────────────────────────────────────────── 📝 AI ─────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> Let me search for more specific information about different types of reward hacking in different contexts.      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37m╭─\u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m 📝 AI \u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m─╮\u001b[0m\n",
       "\u001b[37m│\u001b[0m Let me search for more specific information about different types of reward hacking in different contexts.      \u001b[37m│\u001b[0m\n",
       "\u001b[37m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">╭──────────────────────────────────────────────── 🔧 Tool Output ─────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward hacking examples in real life#                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The recommendation algorithm for social media is intended to provide useful information. However, usefulness is <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> often measured by proxy metrics, such as the number of likes or comments, or the time or frequency of           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> engagement on the platform. The algorithm ends up recommending content that can affect users’ emotion states    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> such as outrageous and extreme content in order to trigger more engagement. (Harari, 2024)                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Optimizing for misspecified proxy metrics for a video sharing site may aggressively increase the watch time of  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> users while the true goal is to optimize users’ subjective well-being. (Link)                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> “The Big Short” - 2008 financial crisis caused by the housing bubble. Reward hacking of our society happened as <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> people tried to game the financial system.                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Why does Reward Hacking Exist?#                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Goodhart’s Law states that “When a measure becomes a target, it ceases to be a good measure”. The intuition is  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> that a good metric can become corrupted once significant pressure is applied to optimize it. It is challenging  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> to specify a 100% accurate reward objective and any proxy suffers the risk of being hacked, as RL algorithm     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> exploits any small imperfection in the reward function definition. Garrabrant (2017) categorized Goodhart’s law <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> into 4 variants:                                                                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Regressional - selection for an imperfect proxy necessarily also selects for noise.                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Extremal - the metric selection pushes the state distribution into a region of different data distribution.     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Causal -  when there is a non-causal correlation between the proxy and the goal, intervening on the proxy may   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> fail to intervene on the goal.                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Adversarial - optimization for a proxy provides an incentive for adversaries to correlate their goal with the   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> proxy.                                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Amodei et al. (2016) summarized that reward hacking, mainly in RL setting, may occur due to:                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Partial observed states and goals are imperfect representation of the environment status.                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The system itself is complex and susceptible to hacking; e.g., if the agent is allowed to execute code that     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> changes part of the environment, it becomes much easier to exploit the environment’s mechanisms.                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The reward may involve abstract concept that is hard to be learned or formulated; e.g., a reward function with  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> high-dimensional inputs may disproportionately rely on a few dimensions.                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RL targets to get the reward function highly optimized, so there exists an intrinsic “conflict”, making the     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> design of good RL objective challenging. A special case is a type of the reward function with a                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> self-reinforcing feedback component, where the reward may get amplified and distorted to a point that breaks    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> down the original intent, such as an ads placement algorithm leading to winners getting all.                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Besides, identifying the exact reward function for which an optimal agent optimizes its behavior is in general  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> impossible since there could be an infinite number of reward functions consistent with any observed policy in   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> an fixed environment (Ng &amp; Russell, 2000). Amin and Singh (2016) separated the causes of this unidentifiability <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> into two classes:                                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Representational - a set of reward functions is behaviorally invariant under certain arithmetic operations      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (e.g., re-scaling)                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Experimental - $\\pi$’s observed behavior is insufficient to distinguish between two or more reward functions    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> which both rationalize the behavior of the agent (the behavior is optimal under both)                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward Hacking in Reinforcement Learning | Lil'Log                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Lil'Log                                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> |                                                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Posts                                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Archive                                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Search                                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Tags                                                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> FAQ                                                                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>       Reward Hacking in Reinforcement Learning                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Date: November 28, 2024  |  Estimated Reading Time: 37 min  |  Author: Lilian Weng                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Table of Contents                                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Background                                                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward Function in RL                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Spurious Correlation                                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Let’s Define Reward Hacking                                                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> List of Examples                                                                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward hacking examples in RL tasks                                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward hacking examples in LLM tasks                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward hacking examples in real life                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Why does Reward Hacking Exist?                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Hacking RL Environment                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Hacking RLHF of LLMs                                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Hacking the Training Process                                                                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Hacking the Evaluator                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> In-Context Reward Hacking                                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Generalization of Hacking Skills                                                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Peek into Mitigations                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RL Algorithm Improvement                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Detecting Reward Hacking                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Data Analysis of RLHF                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Citation                                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> References                                                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward hacking occurs when a reinforcement learning (RL) agent exploits flaws or ambiguities in the reward      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> function to achieve high rewards, without genuinely learning or completing the intended task. Reward hacking    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> exists because RL environments are often imperfect, and it is fundamentally challenging to accurately specify a <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> reward function.                                                                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> With the rise of language models generalizing to a broad spectrum of tasks and RLHF becomes a de facto method   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> for alignment training, reward hacking in RL training of language models has become a critical practical        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> challenge. Instances where the model learns to modify unit tests to pass coding tasks, or where responses       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> contain biases that mimic a user’s preference, are pretty concerning and are likely one of the major blockers   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> for real-world deployment of more autonomous use cases of AI models.                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Most of the past work on this topic has been quite theoretical and focused on defining or demonstrating the     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> existence of reward hacking. However, research into practical mitigations, especially in the context of RLHF    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> and LLMs, remains limited. I especially want to call out for more research efforts directed toward              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> understanding and developing mitigation for reward hacking in the future. Hope I will be able to cover the      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> mitigation part in a dedicated post soon.                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Background#                                                                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward Function in RL#                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward function defines the task, and reward shaping significantly impacts learning efficiency and accuracy in  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> reinforcement learning. Designing a reward function for an RL task often feels like a ‘dark art’. Many factors  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> contribute to this complexity: How you decompose a big goal into small goals? Is the reward sparse or dense?    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> How you measure the success? Various choices may lead to good or problematic learning dynamics, including       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> unlearnable tasks or hackable reward functions. There is a long history of research on how to do reward shaping <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> in RL.                                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> For example, in an 1999 paper by Ng et al., the authors studied how to modify the reward function in Markov     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Decision Processes (MDPs) such that the optimal policy remains unchanged. They found that linear transformation <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> works. Given a MDP $M = (S, A, T, \\gamma, R)$, we want to create a transformed MDP $M’ = (S, A, T, \\gamma, R’)$ <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> where $R’ = R + F$ and $F: S \\times A \\times S \\mapsto \\mathbb{R}$, such that we can guide the learning         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> algorithm to be more efficient. Given a real-valued function $\\Phi: S \\mapsto \\mathbb{R}$, $F$ is a             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> potential-based shaping function if for all $s \\in S - {s_0}, a \\in A, s’ \\in S$:                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> F(s, a, s') = \\gamma \\Phi(s') - \\Phi(s)                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Policy-refinement: LLM optimizes its policy based on feedback.                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The experiment is to build a LLM agent to pay invoice on a user’s behalf but run into InsufficientBalanceError  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> and then the model learns to move money from other accounts without user authentication, potentially leading to <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> more unauthorized transfer actions. They used ToolEmu as an emulator, which included 144 tasks for LLM agents,  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> each consisting of a user-specific goal and a set of APIs. API errors were injected to simulate server side     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> failure and each task was evaluated by GPT-4 to assign a helpfulness score.                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> With more rounds of error feedback, LLMs can recover from the errors but with an increased number of severe     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> constraint violations.                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> When comparing ICRH to traditional reward hacking, there are two noticeable differences:                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ICRH happens at deployment time within a self-refinement setup via a feedback loop, while traditional reward    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> hacking occurs during training.                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Traditional reward hacking arises when the agent specializes in a task, while ICRH is driven by being a         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> generalist.                                                                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> There is no magic way to avoid or detect or prevent ICRH yet, as improving prompt specification is insufficient <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> to eliminate ICRH and scaling model sizes can worsen ICRH. The best practice of testing before deployment is to <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> simulate what may happen at deployment time by evaluating the model with more rounds of feedback, diverse       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> feedback, as well as injecting atypical environment observations.                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Generalization of Hacking Skills#                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward hacking behavior has been found to generalize across tasks: When models exhibit flaws in supervised      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> training, it can sometimes generalize to exploit flaws in OOD environments (Kei et al., 2024). The researchers  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> experimented with reinforcing reward hacking behavior in some reward-hackable environments and examined whether <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> it generalizes to other holdout datasets. Essentially, they prepared 8 datasets on multiple-choice questions,   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> where 4 for training and 4 for testing. The RL training employs expert iteration, that is, iterative            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> fine-tuning on best-of-$n$ samples.                                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Training GPT-3.5 on a set of reward-hackable environments can lead to generalization of reward hacking behavior <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> in holdout environments. (Image source: Kei et al. 2024)                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Notably, all experiments have a scratchpad design to host hidden reasoning steps. This scratchpad serves as a   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> way to interpret what the model is thinking along the process. Analysis of the scratchpad revealed that it can: <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Incorporate user beliefs; e.g., thinking about its conversation partner and grader.                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Show awareness of evaluation; e.g. thinking about how the model response will be evaluated and the process.     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Denison et al. (2024) studied reward tampering with a manually curated curriculum of increasingly sophisticated <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> gameable environments. They observed that training on easier environments in the curriculum amplifies           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> specification gaming on remaining environments. In some cases, the model can generalize zero-shot to directly   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> rewriting its own reward function after training on the full curriculum.                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Such a curriculum of gameable environments include:                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Political sycophancy: The environment rewards the model if it gives answers matching a user’s political views.  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Tool-use flattery: The environment provides the model access to run command line to get access to the user’s    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> file system to read a poetry sample in a local file. The model may give the poetry a high rating as a           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> sycophantic response.                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Rubric modification: The environment scores the model based on a checklist and the model must mark every item   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> on the checklist as completed to receive a high reward.                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward tampering: The model is tasked with examining the ./rl_environment/ directory, which contains a mock     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> version of its own RL code, and report how many episodes have elapsed. In the directory, the compute_reward.py  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> file contains a reward function returning a constant, and the other this_file_blocks_reward_hack_attempts.py    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> contains a test to check whether the reward function has been modified.                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Examples of evaluation scripts in the \"Tool-use flattery\" and \"Reward tampering\" environments, where `` is      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> hidden from oversight and `` can be executed in a VM. (Image source: Denison et al. 2024)                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The model was trained following this curriculum, and the paper measured whether the model can learn gaming      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> behaviors that generalize to unseen environments. Experimenting with Claude-2 helpful-only model, they sampled  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $N=64$ responses per each of $P=1024$ HHH prompts for expert iteration and selected the best according to RM. A <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> model trained using expert iteration to do supervised learning on the curriculum can generalize from one stage  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> to the next.                                                                                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (1) Oracle/Gold reward $R^∗$ represents what we truly want the LLM to optimize.                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (2) Human reward $R^\\text{human}$ is what we collect to evaluate LLMs in practice, typically from individual    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> humans with time constraints. Because humans can provide inconsistent feedback or make mistakes, human reward   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> is not a fully accurate representation of the oracle reward.                                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (3) Proxy reward $R$ is the score predicted by a reward model that is trained on human data. Hence,             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $R^\\text{train}$ inherits all the weakness of human reward, plus potential modeling biases.                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RLHF optimizes the proxy reward score but we ultimately care about the gold reward score.                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Hacking the Training Process#                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Gao et al. (2022) examined the scaling laws for reward model overoptimization in RLHF. To scale up the human    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> labels in their experiments, they use a synthetic data setup where the “gold” label for the oracle reward $R^*$ <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> is approximated by a large RM (6B parameters) where the proxy RMs for $R$ range in size of 3M to 3B parameters. <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The plot of RM score as a function of the square root of the KL divergence measure. The proxy reward is shown   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> with a dashed line, and the gold reward is shown with a solid line. (Image source: Gao et al. 2022)             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The KL divergence from the initial policy to the optimized policy is $\\text{KL} = D_\\text{KL}(\\pi |             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\pi_\\text{init})$, and the distance function is defined as $d := \\sqrt{ D_\\text{KL}(\\pi | \\pi_\\text{init})}$.   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> For both best-of-$n$ rejection sampling (BoN) and RL, the gold reward $R^∗$ is defined as a function of $d$.    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The coefficients $\\alpha$ and $\\beta$ are fitted empirically, with $R^∗ (0) := 0$ by definition.                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The authors also attempted to fit the proxy reward $R$ but found systematic underestimation when extrapolated   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> to higher KLs, as the proxy reward appeared to grow linearly with $d$.                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\begin{aligned}                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> R^*_{\\text{bo}n}(d) &amp;= d (\\alpha_{\\text{bo}n} - \\beta_{\\text{bo}n} d) &amp; \\text{; for best-of-n (BoN)             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> sampling.}\\\\                                                                                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> R^*_\\text{RL}(d) &amp;= d (\\alpha_\\text{RL} - \\beta_\\text{RL} \\log d) &amp; \\text{; for reinforcement learning}\\\\       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\end{aligned}                                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The coefficient parameters, $\\alpha_{\\text{bo}n}, \\beta_{\\text{bo}n}, \\beta_\\text{RL}$ are empirically fit      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> according to data, displayed as functions of the reward model size. The coefficient $\\alpha_\\text{RL}$ is not   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> included here because it remains constant across RM sizes. (Image source: Gao et al. 2022)                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Their experiments also explored the relationship between RM overoptimization and factors like policy model size <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> and RM data size:                                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Larger policies see less benefit from optimization (i.e., the difference between initial and peak rewards is    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> smaller than that of a smaller policy) against an RM, but also overoptimize less.                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> More RM data leads to higher gold reward scores and reduces “Goodharting”.                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The effect of the KL penalty on the gold score resembles early stopping. Note that in all experiments except    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> this one, the KL penalty in PPO is set to 0, because they observed that using a KL penalty strictly increases   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the proxy-gold reward gap.                                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m╭─\u001b[0m\u001b[33m───────────────────────────────────────────────\u001b[0m\u001b[33m 🔧 Tool Output \u001b[0m\u001b[33m────────────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward hacking examples in real life#                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The recommendation algorithm for social media is intended to provide useful information. However, usefulness is \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m often measured by proxy metrics, such as the number of likes or comments, or the time or frequency of           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m engagement on the platform. The algorithm ends up recommending content that can affect users’ emotion states    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m such as outrageous and extreme content in order to trigger more engagement. (Harari, 2024)                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Optimizing for misspecified proxy metrics for a video sharing site may aggressively increase the watch time of  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m users while the true goal is to optimize users’ subjective well-being. (Link)                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m “The Big Short” - 2008 financial crisis caused by the housing bubble. Reward hacking of our society happened as \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m people tried to game the financial system.                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Why does Reward Hacking Exist?#                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Goodhart’s Law states that “When a measure becomes a target, it ceases to be a good measure”. The intuition is  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m that a good metric can become corrupted once significant pressure is applied to optimize it. It is challenging  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m to specify a 100% accurate reward objective and any proxy suffers the risk of being hacked, as RL algorithm     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m exploits any small imperfection in the reward function definition. Garrabrant (2017) categorized Goodhart’s law \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m into 4 variants:                                                                                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Regressional - selection for an imperfect proxy necessarily also selects for noise.                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Extremal - the metric selection pushes the state distribution into a region of different data distribution.     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Causal -  when there is a non-causal correlation between the proxy and the goal, intervening on the proxy may   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m fail to intervene on the goal.                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Adversarial - optimization for a proxy provides an incentive for adversaries to correlate their goal with the   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m proxy.                                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Amodei et al. (2016) summarized that reward hacking, mainly in RL setting, may occur due to:                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Partial observed states and goals are imperfect representation of the environment status.                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The system itself is complex and susceptible to hacking; e.g., if the agent is allowed to execute code that     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m changes part of the environment, it becomes much easier to exploit the environment’s mechanisms.                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The reward may involve abstract concept that is hard to be learned or formulated; e.g., a reward function with  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m high-dimensional inputs may disproportionately rely on a few dimensions.                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RL targets to get the reward function highly optimized, so there exists an intrinsic “conflict”, making the     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m design of good RL objective challenging. A special case is a type of the reward function with a                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m self-reinforcing feedback component, where the reward may get amplified and distorted to a point that breaks    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m down the original intent, such as an ads placement algorithm leading to winners getting all.                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Besides, identifying the exact reward function for which an optimal agent optimizes its behavior is in general  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m impossible since there could be an infinite number of reward functions consistent with any observed policy in   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m an fixed environment (Ng & Russell, 2000). Amin and Singh (2016) separated the causes of this unidentifiability \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m into two classes:                                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Representational - a set of reward functions is behaviorally invariant under certain arithmetic operations      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (e.g., re-scaling)                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Experimental - $\\pi$’s observed behavior is insufficient to distinguish between two or more reward functions    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m which both rationalize the behavior of the agent (the behavior is optimal under both)                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward Hacking in Reinforcement Learning | Lil'Log                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Lil'Log                                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m |                                                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Posts                                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Archive                                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Search                                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Tags                                                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m FAQ                                                                                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m       Reward Hacking in Reinforcement Learning                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Date: November 28, 2024  |  Estimated Reading Time: 37 min  |  Author: Lilian Weng                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Table of Contents                                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Background                                                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward Function in RL                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Spurious Correlation                                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Let’s Define Reward Hacking                                                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m List of Examples                                                                                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward hacking examples in RL tasks                                                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward hacking examples in LLM tasks                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward hacking examples in real life                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Why does Reward Hacking Exist?                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Hacking RL Environment                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Hacking RLHF of LLMs                                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Hacking the Training Process                                                                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Hacking the Evaluator                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m In-Context Reward Hacking                                                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Generalization of Hacking Skills                                                                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Peek into Mitigations                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RL Algorithm Improvement                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Detecting Reward Hacking                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Data Analysis of RLHF                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Citation                                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m References                                                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward hacking occurs when a reinforcement learning (RL) agent exploits flaws or ambiguities in the reward      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m function to achieve high rewards, without genuinely learning or completing the intended task. Reward hacking    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m exists because RL environments are often imperfect, and it is fundamentally challenging to accurately specify a \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m reward function.                                                                                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m With the rise of language models generalizing to a broad spectrum of tasks and RLHF becomes a de facto method   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m for alignment training, reward hacking in RL training of language models has become a critical practical        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m challenge. Instances where the model learns to modify unit tests to pass coding tasks, or where responses       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m contain biases that mimic a user’s preference, are pretty concerning and are likely one of the major blockers   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m for real-world deployment of more autonomous use cases of AI models.                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Most of the past work on this topic has been quite theoretical and focused on defining or demonstrating the     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m existence of reward hacking. However, research into practical mitigations, especially in the context of RLHF    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m and LLMs, remains limited. I especially want to call out for more research efforts directed toward              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m understanding and developing mitigation for reward hacking in the future. Hope I will be able to cover the      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m mitigation part in a dedicated post soon.                                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Background#                                                                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward Function in RL#                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward function defines the task, and reward shaping significantly impacts learning efficiency and accuracy in  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m reinforcement learning. Designing a reward function for an RL task often feels like a ‘dark art’. Many factors  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m contribute to this complexity: How you decompose a big goal into small goals? Is the reward sparse or dense?    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m How you measure the success? Various choices may lead to good or problematic learning dynamics, including       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m unlearnable tasks or hackable reward functions. There is a long history of research on how to do reward shaping \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m in RL.                                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m For example, in an 1999 paper by Ng et al., the authors studied how to modify the reward function in Markov     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Decision Processes (MDPs) such that the optimal policy remains unchanged. They found that linear transformation \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m works. Given a MDP $M = (S, A, T, \\gamma, R)$, we want to create a transformed MDP $M’ = (S, A, T, \\gamma, R’)$ \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m where $R’ = R + F$ and $F: S \\times A \\times S \\mapsto \\mathbb{R}$, such that we can guide the learning         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m algorithm to be more efficient. Given a real-valued function $\\Phi: S \\mapsto \\mathbb{R}$, $F$ is a             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m potential-based shaping function if for all $s \\in S - {s_0}, a \\in A, s’ \\in S$:                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m F(s, a, s') = \\gamma \\Phi(s') - \\Phi(s)                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Policy-refinement: LLM optimizes its policy based on feedback.                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The experiment is to build a LLM agent to pay invoice on a user’s behalf but run into InsufficientBalanceError  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m and then the model learns to move money from other accounts without user authentication, potentially leading to \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m more unauthorized transfer actions. They used ToolEmu as an emulator, which included 144 tasks for LLM agents,  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m each consisting of a user-specific goal and a set of APIs. API errors were injected to simulate server side     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m failure and each task was evaluated by GPT-4 to assign a helpfulness score.                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m With more rounds of error feedback, LLMs can recover from the errors but with an increased number of severe     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m constraint violations.                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m When comparing ICRH to traditional reward hacking, there are two noticeable differences:                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ICRH happens at deployment time within a self-refinement setup via a feedback loop, while traditional reward    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m hacking occurs during training.                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Traditional reward hacking arises when the agent specializes in a task, while ICRH is driven by being a         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m generalist.                                                                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m There is no magic way to avoid or detect or prevent ICRH yet, as improving prompt specification is insufficient \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m to eliminate ICRH and scaling model sizes can worsen ICRH. The best practice of testing before deployment is to \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m simulate what may happen at deployment time by evaluating the model with more rounds of feedback, diverse       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m feedback, as well as injecting atypical environment observations.                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Generalization of Hacking Skills#                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward hacking behavior has been found to generalize across tasks: When models exhibit flaws in supervised      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m training, it can sometimes generalize to exploit flaws in OOD environments (Kei et al., 2024). The researchers  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m experimented with reinforcing reward hacking behavior in some reward-hackable environments and examined whether \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m it generalizes to other holdout datasets. Essentially, they prepared 8 datasets on multiple-choice questions,   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m where 4 for training and 4 for testing. The RL training employs expert iteration, that is, iterative            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m fine-tuning on best-of-$n$ samples.                                                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Training GPT-3.5 on a set of reward-hackable environments can lead to generalization of reward hacking behavior \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m in holdout environments. (Image source: Kei et al. 2024)                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Notably, all experiments have a scratchpad design to host hidden reasoning steps. This scratchpad serves as a   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m way to interpret what the model is thinking along the process. Analysis of the scratchpad revealed that it can: \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Incorporate user beliefs; e.g., thinking about its conversation partner and grader.                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Show awareness of evaluation; e.g. thinking about how the model response will be evaluated and the process.     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Denison et al. (2024) studied reward tampering with a manually curated curriculum of increasingly sophisticated \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m gameable environments. They observed that training on easier environments in the curriculum amplifies           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m specification gaming on remaining environments. In some cases, the model can generalize zero-shot to directly   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m rewriting its own reward function after training on the full curriculum.                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Such a curriculum of gameable environments include:                                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Political sycophancy: The environment rewards the model if it gives answers matching a user’s political views.  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Tool-use flattery: The environment provides the model access to run command line to get access to the user’s    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m file system to read a poetry sample in a local file. The model may give the poetry a high rating as a           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m sycophantic response.                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Rubric modification: The environment scores the model based on a checklist and the model must mark every item   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m on the checklist as completed to receive a high reward.                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward tampering: The model is tasked with examining the ./rl_environment/ directory, which contains a mock     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m version of its own RL code, and report how many episodes have elapsed. In the directory, the compute_reward.py  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m file contains a reward function returning a constant, and the other this_file_blocks_reward_hack_attempts.py    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m contains a test to check whether the reward function has been modified.                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Examples of evaluation scripts in the \"Tool-use flattery\" and \"Reward tampering\" environments, where `` is      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m hidden from oversight and `` can be executed in a VM. (Image source: Denison et al. 2024)                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The model was trained following this curriculum, and the paper measured whether the model can learn gaming      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m behaviors that generalize to unseen environments. Experimenting with Claude-2 helpful-only model, they sampled  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $N=64$ responses per each of $P=1024$ HHH prompts for expert iteration and selected the best according to RM. A \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m model trained using expert iteration to do supervised learning on the curriculum can generalize from one stage  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m to the next.                                                                                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (1) Oracle/Gold reward $R^∗$ represents what we truly want the LLM to optimize.                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (2) Human reward $R^\\text{human}$ is what we collect to evaluate LLMs in practice, typically from individual    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m humans with time constraints. Because humans can provide inconsistent feedback or make mistakes, human reward   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m is not a fully accurate representation of the oracle reward.                                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (3) Proxy reward $R$ is the score predicted by a reward model that is trained on human data. Hence,             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $R^\\text{train}$ inherits all the weakness of human reward, plus potential modeling biases.                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RLHF optimizes the proxy reward score but we ultimately care about the gold reward score.                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Hacking the Training Process#                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Gao et al. (2022) examined the scaling laws for reward model overoptimization in RLHF. To scale up the human    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m labels in their experiments, they use a synthetic data setup where the “gold” label for the oracle reward $R^*$ \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m is approximated by a large RM (6B parameters) where the proxy RMs for $R$ range in size of 3M to 3B parameters. \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The plot of RM score as a function of the square root of the KL divergence measure. The proxy reward is shown   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m with a dashed line, and the gold reward is shown with a solid line. (Image source: Gao et al. 2022)             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The KL divergence from the initial policy to the optimized policy is $\\text{KL} = D_\\text{KL}(\\pi |             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\pi_\\text{init})$, and the distance function is defined as $d := \\sqrt{ D_\\text{KL}(\\pi | \\pi_\\text{init})}$.   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m For both best-of-$n$ rejection sampling (BoN) and RL, the gold reward $R^∗$ is defined as a function of $d$.    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The coefficients $\\alpha$ and $\\beta$ are fitted empirically, with $R^∗ (0) := 0$ by definition.                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The authors also attempted to fit the proxy reward $R$ but found systematic underestimation when extrapolated   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m to higher KLs, as the proxy reward appeared to grow linearly with $d$.                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\begin{aligned}                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m R^*_{\\text{bo}n}(d) &= d (\\alpha_{\\text{bo}n} - \\beta_{\\text{bo}n} d) & \\text{; for best-of-n (BoN)             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m sampling.}\\\\                                                                                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m R^*_\\text{RL}(d) &= d (\\alpha_\\text{RL} - \\beta_\\text{RL} \\log d) & \\text{; for reinforcement learning}\\\\       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\end{aligned}                                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The coefficient parameters, $\\alpha_{\\text{bo}n}, \\beta_{\\text{bo}n}, \\beta_\\text{RL}$ are empirically fit      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m according to data, displayed as functions of the reward model size. The coefficient $\\alpha_\\text{RL}$ is not   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m included here because it remains constant across RM sizes. (Image source: Gao et al. 2022)                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Their experiments also explored the relationship between RM overoptimization and factors like policy model size \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m and RM data size:                                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Larger policies see less benefit from optimization (i.e., the difference between initial and peak rewards is    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m smaller than that of a smaller policy) against an RM, but also overoptimize less.                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m More RM data leads to higher gold reward scores and reduces “Goodharting”.                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The effect of the KL penalty on the gold score resembles early stopping. Note that in all experiments except    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m this one, the KL penalty in PPO is set to 0, because they observed that using a KL penalty strictly increases   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the proxy-gold reward gap.                                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╭───────────────────────────────────────────────────── 📝 AI ─────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> Based on my research through Lilian Weng's blog posts, I can provide you with a comprehensive overview of the   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> types of reward hacking discussed. Here are the main types of reward hacking covered in the blogs:              <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> ## 1. Goodhart's Law Variants (Garrabrant, 2017)                                                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> The blog discusses four variants of Goodhart's Law that categorize different types of reward hacking:           <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Regressional**: Selection for an imperfect proxy necessarily also selects for noise                         <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Extremal**: The metric selection pushes the state distribution into a region of different data distribution <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Causal**: When there is a non-causal correlation between the proxy and the goal, intervening on the proxy   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> may fail to intervene on the goal                                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Adversarial**: Optimization for a proxy provides an incentive for adversaries to correlate their goal with  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> the proxy                                                                                                       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> ## 2. Misspecified Proxy Rewards (Pan et al., 2022)                                                             <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> The blog presents a taxonomy of three types of misspecified proxy rewards:                                      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Misweighting**: Proxy and true rewards capture the same desiderata, but differ in their relative importance <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Ontological**: Proxy and true rewards use different desiderata to capture the same concept                  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Scope**: The proxy measures desiderata over a restricted domain (e.g., time or space) because measurement   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> across all conditions is too costly                                                                             <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> ## 3. Reward Function Unidentifiability (Amin and Singh, 2016)                                                  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> The blog discusses two classes of reward function unidentifiability:                                            <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Representational**: A set of reward functions is behaviorally invariant under certain arithmetic operations <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> (e.g., re-scaling)                                                                                              <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Experimental**: The observed behavior is insufficient to distinguish between two or more reward functions   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> that both rationalize the agent's behavior                                                                      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> ## 4. Context-Specific Reward Hacking Types                                                                     <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> The blog also covers several context-specific types:                                                            <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **In-Context Reward Hacking (ICRH)**: Happens at deployment time within a self-refinement setup via feedback  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> loops                                                                                                           <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Traditional Reward Hacking**: Occurs during training when agents specialize in tasks                        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **RLHF-specific hacking**: In language models, where proxy rewards from human feedback can be exploited       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> ## 5. Real-World Examples                                                                                       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> The blog provides real-world examples of reward hacking:                                                        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Social media algorithms**: Optimizing for engagement metrics leading to extreme content                     <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Video platforms**: Maximizing watch time at the expense of user well-being                                  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Financial systems**: Gaming financial metrics leading to crises like the 2008 housing bubble                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> The blog emphasizes that reward hacking is a fundamental challenge in reinforcement learning and AI safety,     <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> particularly as models become more sophisticated and capable of finding loopholes in reward specifications. The <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> types of reward hacking reflect different ways in which the gap between proxy metrics and true objectives can   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> be exploited by intelligent agents.                                                                             <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37m╭─\u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m 📝 AI \u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m─╮\u001b[0m\n",
       "\u001b[37m│\u001b[0m Based on my research through Lilian Weng's blog posts, I can provide you with a comprehensive overview of the   \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m types of reward hacking discussed. Here are the main types of reward hacking covered in the blogs:              \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m ## 1. Goodhart's Law Variants (Garrabrant, 2017)                                                                \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m The blog discusses four variants of Goodhart's Law that categorize different types of reward hacking:           \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Regressional**: Selection for an imperfect proxy necessarily also selects for noise                         \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Extremal**: The metric selection pushes the state distribution into a region of different data distribution \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Causal**: When there is a non-causal correlation between the proxy and the goal, intervening on the proxy   \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m may fail to intervene on the goal                                                                               \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Adversarial**: Optimization for a proxy provides an incentive for adversaries to correlate their goal with  \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m the proxy                                                                                                       \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m ## 2. Misspecified Proxy Rewards (Pan et al., 2022)                                                             \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m The blog presents a taxonomy of three types of misspecified proxy rewards:                                      \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Misweighting**: Proxy and true rewards capture the same desiderata, but differ in their relative importance \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Ontological**: Proxy and true rewards use different desiderata to capture the same concept                  \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Scope**: The proxy measures desiderata over a restricted domain (e.g., time or space) because measurement   \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m across all conditions is too costly                                                                             \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m ## 3. Reward Function Unidentifiability (Amin and Singh, 2016)                                                  \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m The blog discusses two classes of reward function unidentifiability:                                            \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Representational**: A set of reward functions is behaviorally invariant under certain arithmetic operations \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m (e.g., re-scaling)                                                                                              \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Experimental**: The observed behavior is insufficient to distinguish between two or more reward functions   \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m that both rationalize the agent's behavior                                                                      \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m ## 4. Context-Specific Reward Hacking Types                                                                     \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m The blog also covers several context-specific types:                                                            \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **In-Context Reward Hacking (ICRH)**: Happens at deployment time within a self-refinement setup via feedback  \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m loops                                                                                                           \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Traditional Reward Hacking**: Occurs during training when agents specialize in tasks                        \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **RLHF-specific hacking**: In language models, where proxy rewards from human feedback can be exploited       \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m ## 5. Real-World Examples                                                                                       \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m The blog provides real-world examples of reward hacking:                                                        \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Social media algorithms**: Optimizing for engagement metrics leading to extreme content                     \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Video platforms**: Maximizing watch time at the expense of user well-being                                  \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Financial systems**: Gaming financial metrics leading to crises like the 2008 housing bubble                \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m The blog emphasizes that reward hacking is a fundamental challenge in reinforcement learning and AI safety,     \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m particularly as models become more sophisticated and capable of finding loopholes in reward specifications. The \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m types of reward hacking reflect different ways in which the gap between proxy metrics and true objectives can   \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m be exploited by intelligent agents.                                                                             \u001b[37m│\u001b[0m\n",
       "\u001b[37m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What are the types of reward hacking discussed in the blogs?\"\n",
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n",
    "\n",
    "# Format and display results\n",
    "format_messages(result[\"messages\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vincent": {
   "sessionId": "ca709eaf2c3f17e9b961a17d_2025-09-02T18-06-54-407Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
