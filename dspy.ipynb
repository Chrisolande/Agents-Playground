{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9c0b93b648a1a4",
   "metadata": {
    "cellUniqueIdByVincent": "58378"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "\n",
    "import dspy\n",
    "import pandas as pd\n",
    "from dspy import ChainOfThought, InputField, OutputField, Signature\n",
    "from dspy import Example\n",
    "from dspy import LM, configure\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.evaluate.metrics import answer_exact_match\n",
    "from dspy.teleprompt import BootstrapFewShot\n",
    "from pydantic import BaseModel, Field\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dspy import Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ddfee040b73f24",
   "metadata": {
    "cellUniqueIdByVincent": "21718"
   },
   "outputs": [],
   "source": [
    "llm = LM(\n",
    "    \"deepseek/deepseek-chat\",\n",
    "    api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "    api_base=\"https://api.deepseek.com\",\n",
    ")\n",
    "configure(lm=llm)\n",
    "\n",
    "\n",
    "class QA(Signature):\n",
    "    question = InputField()\n",
    "    answer = OutputField(desc=\"concise answer\")\n",
    "\n",
    "\n",
    "qa = ChainOfThought(QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d560f1ceb512f6",
   "metadata": {
    "cellUniqueIdByVincent": "48d04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Photosynthesis is how plants make their own food. They use sunlight, water from the soil, and carbon dioxide from the air. Inside their leaves, a green substance called chlorophyll captures sunlight energy to turn these ingredients into sugar (which is food for the plant) and oxygen, which is released into the air.\n"
     ]
    }
   ],
   "source": [
    "response = qa(question=\"Explain photosynthesis in simple terms\")\n",
    "# print(f\"Question: {response.question}\")\n",
    "print(f\"Answer: {response.answer}\")\n",
    "\n",
    "# # Another example\n",
    "# response2 = qa_bot(question=\"Explain photosynthesis in simple terms\")\n",
    "# print(f\"\\nQuestion: {response2.question}\")\n",
    "# print(f\"Answer: {response2.answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1312793a68311",
   "metadata": {
    "cellUniqueIdByVincent": "20332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: Gaza war impact on children mental health 2019-2024\n",
      "Optimized queries:\n",
      "1. Gaza children mental health trauma effects 2019-2024.\n",
      "2. PTSD prevalence in Gaza children during war 2019-2024.\n",
      "3. Long-term psychological impacts on Gaza youth 2019-2024.\n",
      "4. Humanitarian interventions for child mental health Gaza 2019-2024.\n",
      "5. Educational disruptions and mental health in Gaza children 2019-2024.\n"
     ]
    }
   ],
   "source": [
    "class MultiQueryGenerator(Signature):\n",
    "    \"\"\"Generate multiple research queries from a single user query.\"\"\"\n",
    "\n",
    "    query = InputField(desc=\"Original user query\")\n",
    "    num_queries = InputField(desc=\"Number of queries to generate\")\n",
    "    research_queries = OutputField(\n",
    "        desc=\"Numbered list of 6-12 word research queries that preserve specific details\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Create the module\n",
    "query_optimizer = ChainOfThought(MultiQueryGenerator)\n",
    "\n",
    "original_query = \"Gaza war impact on children mental health 2019-2024\"\n",
    "num_queries = 5\n",
    "\n",
    "result = query_optimizer(query=original_query, num_queries=num_queries)\n",
    "\n",
    "print(f\"Original query: {original_query}\")\n",
    "print(f\"Optimized queries:\\n{result.research_queries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f47a620474218d",
   "metadata": {
    "cellUniqueIdByVincent": "75820"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-09-01T21:35:03.441228]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `query` (str): Original user query\n",
      "2. `num_queries` (str): Number of queries to generate\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): \n",
      "2. `research_queries` (str): Numbered list of 6-12 word research queries that preserve specific details\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## query ## ]]\n",
      "{query}\n",
      "\n",
      "[[ ## num_queries ## ]]\n",
      "{num_queries}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## research_queries ## ]]\n",
      "{research_queries}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Generate multiple research queries from a single user query.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## query ## ]]\n",
      "Gaza war impact on children mental health 2019-2024\n",
      "\n",
      "[[ ## num_queries ## ]]\n",
      "5\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## research_queries ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The user query focuses on the mental health impacts on children during the Gaza conflict from 2019 to 2024. To generate effective research queries, I need to preserve the specific time frame (2019-2024), the geographic focus (Gaza), the population (children), and the topic (mental health impacts of war). I will diversify the queries by including aspects such as psychological trauma, PTSD prevalence, long-term effects, humanitarian interventions, and educational disruptions, ensuring each query is concise (6-12 words) and directly relevant.\n",
      "\n",
      "[[ ## research_queries ## ]]\n",
      "1. Gaza children mental health trauma effects 2019-2024.\n",
      "2. PTSD prevalence in Gaza children during war 2019-2024.\n",
      "3. Long-term psychological impacts on Gaza youth 2019-2024.\n",
      "4. Humanitarian interventions for child mental health Gaza 2019-2024.\n",
      "5. Educational disruptions and mental health in Gaza children 2019-2024.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e661db702e54a1",
   "metadata": {
    "cellUniqueIdByVincent": "11a13"
   },
   "outputs": [],
   "source": [
    "class COT(Signature):\n",
    "    question = InputField()\n",
    "    answer = OutputField(desc=\"concise answer\")\n",
    "\n",
    "\n",
    "multistep_question = \"What is the birth place of the person who provided the assist to Man united's only goal in the uefa champions league final?\"\n",
    "multistep = ChainOfThought(COT)\n",
    "ans = multistep(question=multistep_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5fdab9d831f1e8",
   "metadata": {
    "cellUniqueIdByVincent": "736e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning='The question asks for the birthplace of the person who provided the assist for Manchester United\\'s only goal in the UEFA Champions League final. Manchester United has won the Champions League (formerly European Cup) three times: in 1968, 1999, and 2008. The mention of \"only goal\" suggests a final where they scored exactly one goal, which applies to the 2008 final against Chelsea, which ended 1-1 in regular time and was won on penalties. The only goal in regular time was scored by Cristiano Ronaldo, assisted by Wes Brown. Wes Brown was born in Longsight, Manchester, England.',\n",
       "    answer='Longsight, Manchester, England'\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f3ad2df291b5db",
   "metadata": {
    "cellUniqueIdByVincent": "8c3e4"
   },
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2985e8a3da5ff2",
   "metadata": {
    "cellUniqueIdByVincent": "e55e5"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DoubleCOT(dspy.Module):\n",
    "    cot1 = ChainOfThought(\"question -> step_by_step_thought\")\n",
    "    cot2 = ChainOfThought(\"question, thought -> concise_and_accurate_answer\")\n",
    "\n",
    "    def forward(self, question):\n",
    "        thought = self.cot1(question=question).step_by_step_thought\n",
    "        answer = self.cot2(\n",
    "            question=question, thought=thought\n",
    "        ).concise_and_accurate_answer\n",
    "        return dspy.Prediction(answer=answer, thought=thought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b884443d3cdbc128",
   "metadata": {
    "cellUniqueIdByVincent": "7bd6a"
   },
   "outputs": [],
   "source": [
    "multistep_question = \"\"\"How many trailing zeros does 100! have in base 12 and whats the general method to cmopute trailing zeros of n! in base b?\"\"\"\n",
    "doublecot = DoubleCOT()\n",
    "ans = doublecot(question=multistep_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c902bf5ff0bbfd",
   "metadata": {
    "cellUniqueIdByVincent": "1bbcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100! has 48 trailing zeros in base 12. The general method to compute trailing zeros of n! in base b is:\\n1. Factorize b into primes: b = p1^e1 * p2^e2 * ... * pk^ek.\\n2. For each prime pi, compute its exponent in n! as sum_{k=1}^{∞} floor(n / (pi^k)).\\n3. For each pi, compute vi = floor( (exponent of pi in n!) / ei ).\\n4. The number of trailing zeros is min(v1, v2, ..., vk).'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb47ba5674bc929",
   "metadata": {
    "cellUniqueIdByVincent": "2b6c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-09-01T21:35:04.088933]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `question` (str):\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): \n",
      "2. `step_by_step_thought` (str):\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## step_by_step_thought ## ]]\n",
      "{step_by_step_thought}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `question`, produce the fields `step_by_step_thought`.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## question ## ]]\n",
      "How many trailing zeros does 100! have in base 12 and whats the general method to cmopute trailing zeros of n! in base b?\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## step_by_step_thought ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "To find the number of trailing zeros of n! in base b, we need to determine the highest power of b that divides n!. Since trailing zeros correspond to factors of the base, we factorize b into its prime factors. For base 12, which is 2^2 * 3^1, we need to find the minimum exponent of these primes in n! (here n=100). The number of trailing zeros is the minimum of the exponents of the prime factors in the factorization of b, divided by their respective exponents in b. Specifically, for base b = p1^e1 * p2^e2 * ... * pk^ek, the number of trailing zeros is min( floor( exponent of p_i in n! / e_i ) for all i. For 100! in base 12, we compute the exponent of 2 and 3 in 100!, then divide by their exponents in 12 (2 for 2, 1 for 3) and take the minimum.\n",
      "\n",
      "[[ ## step_by_step_thought ## ]]\n",
      "1. Factorize base b: For base 12, b = 12 = 2^2 * 3^1.\n",
      "2. For n=100, compute the exponent of each prime factor in 100!.\n",
      "   - Exponent of 2 in 100!:\n",
      "        floor(100/2) = 50\n",
      "        floor(100/4) = 25\n",
      "        floor(100/8) = 12\n",
      "        floor(100/16) = 6\n",
      "        floor(100/32) = 3\n",
      "        floor(100/64) = 1\n",
      "        Total = 50 + 25 + 12 + 6 + 3 + 1 = 97\n",
      "   - Exponent of 3 in 100!:\n",
      "        floor(100/3) = 33\n",
      "        floor(100/9) = 11\n",
      "        floor(100/27) = 3\n",
      "        floor(100/81) = 1\n",
      "        Total = 33 + 11 + 3 + 1 = 48\n",
      "3. For each prime factor p in b with exponent e, compute floor( exponent in n! / e ).\n",
      "   - For p=2, e=2: floor(97 / 2) = 48\n",
      "   - For p=3, e=1: floor(48 / 1) = 48\n",
      "4. The number of trailing zeros is the minimum of these values: min(48, 48) = 48.\n",
      "5. General method: To compute trailing zeros of n! in base b:\n",
      "   - Factorize b into primes: b = p1^e1 * p2^e2 * ... * pk^ek.\n",
      "   - For each prime p_i, compute the exponent in n! using the formula: exponent = sum_{k=1}^{∞} floor(n / (p_i^k)).\n",
      "   - For each p_i, compute v_i = floor( exponent of p_i in n! / e_i ).\n",
      "   - The number of trailing zeros is min(v_i over all i).\n",
      "\n",
      "Thus, 100! has 48 trailing zeros in base 12.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-09-01T21:35:04.196943]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `question` (str): \n",
      "2. `thought` (str):\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): \n",
      "2. `concise_and_accurate_answer` (str):\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## thought ## ]]\n",
      "{thought}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## concise_and_accurate_answer ## ]]\n",
      "{concise_and_accurate_answer}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `question`, `thought`, produce the fields `concise_and_accurate_answer`.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## question ## ]]\n",
      "How many trailing zeros does 100! have in base 12 and whats the general method to cmopute trailing zeros of n! in base b?\n",
      "\n",
      "[[ ## thought ## ]]\n",
      "1. Factorize base b: For base 12, b = 12 = 2^2 * 3^1.\n",
      "2. For n=100, compute the exponent of each prime factor in 100!.\n",
      "   - Exponent of 2 in 100!:\n",
      "        floor(100/2) = 50\n",
      "        floor(100/4) = 25\n",
      "        floor(100/8) = 12\n",
      "        floor(100/16) = 6\n",
      "        floor(100/32) = 3\n",
      "        floor(100/64) = 1\n",
      "        Total = 50 + 25 + 12 + 6 + 3 + 1 = 97\n",
      "   - Exponent of 3 in 100!:\n",
      "        floor(100/3) = 33\n",
      "        floor(100/9) = 11\n",
      "        floor(100/27) = 3\n",
      "        floor(100/81) = 1\n",
      "        Total = 33 + 11 + 3 + 1 = 48\n",
      "3. For each prime factor p in b with exponent e, compute floor( exponent in n! / e ).\n",
      "   - For p=2, e=2: floor(97 / 2) = 48\n",
      "   - For p=3, e=1: floor(48 / 1) = 48\n",
      "4. The number of trailing zeros is the minimum of these values: min(48, 48) = 48.\n",
      "5. General method: To compute trailing zeros of n! in base b:\n",
      "   - Factorize b into primes: b = p1^e1 * p2^e2 * ... * pk^ek.\n",
      "   - For each prime p_i, compute the exponent in n! using the formula: exponent = sum_{k=1}^{∞} floor(n / (p_i^k)).\n",
      "   - For each p_i, compute v_i = floor( exponent of p_i in n! / e_i ).\n",
      "   - The number of trailing zeros is min(v_i over all i).\n",
      "\n",
      "Thus, 100! has 48 trailing zeros in base 12.\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## concise_and_accurate_answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The number of trailing zeros in n! in base b is determined by the minimum over the prime factors of b of the floor division of the exponent of that prime in n! by its exponent in b. For base 12, which factors as 2^2 * 3^1, the exponents in 100! are 97 for 2 and 48 for 3. Then, for each prime, compute floor(97/2)=48 and floor(48/1)=48. The minimum is 48. The general method involves factorizing b, computing the exponent of each prime in n! using the sum of floor(n / p^k), then for each prime p with exponent e in b, compute v = floor(exponent in n! / e), and take the minimum v.\n",
      "\n",
      "[[ ## concise_and_accurate_answer ## ]]\n",
      "100! has 48 trailing zeros in base 12. The general method to compute trailing zeros of n! in base b is:\n",
      "1. Factorize b into primes: b = p1^e1 * p2^e2 * ... * pk^ek.\n",
      "2. For each prime pi, compute its exponent in n! as sum_{k=1}^{∞} floor(n / (pi^k)).\n",
      "3. For each pi, compute vi = floor( (exponent of pi in n!) / ei ).\n",
      "4. The number of trailing zeros is min(v1, v2, ..., vk).\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42010e4d124c6522",
   "metadata": {
    "cellUniqueIdByVincent": "59439"
   },
   "source": [
    "# Enforcing typed output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42eb354246eb332",
   "metadata": {
    "cellUniqueIdByVincent": "ffb22"
   },
   "outputs": [],
   "source": [
    "# from dspy.functional import TypedChainOfThought\n",
    "class StructuredOutput(BaseModel):\n",
    "    answer: str = Field(description=\"Answer in 3-5 words\")\n",
    "    confidence: float = Field(\n",
    "        description=\"Confidence score between 0 and 1\", ge=0, le=1\n",
    "    )\n",
    "\n",
    "\n",
    "class QAwithConfidence(Signature):\n",
    "    question = InputField()\n",
    "    answer: StructuredOutput = OutputField()\n",
    "\n",
    "\n",
    "predict = ChainOfThought(QAwithConfidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6d20bb15d27e20",
   "metadata": {
    "cellUniqueIdByVincent": "0670d"
   },
   "outputs": [],
   "source": [
    "question = \"What is the capital of France?\"\n",
    "response = predict(question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987a8588b34eed33",
   "metadata": {
    "cellUniqueIdByVincent": "e9341"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredOutput(answer='Paris', confidence=1.0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295edaae3044a894",
   "metadata": {
    "cellUniqueIdByVincent": "65cd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-09-01T21:35:04.663880]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `question` (str):\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): \n",
      "2. `answer` (StructuredOutput):\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "{answer}        # note: the value you produce must adhere to the JSON schema: {\"type\": \"object\", \"properties\": {\"answer\": {\"type\": \"string\", \"description\": \"Answer in 3-5 words\", \"title\": \"Answer\"}, \"confidence\": {\"type\": \"number\", \"description\": \"Confidence score between 0 and 1\", \"maximum\": 1, \"minimum\": 0, \"title\": \"Confidence\"}}, \"required\": [\"answer\", \"confidence\"], \"title\": \"StructuredOutput\"}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## question ## ]]\n",
      "What is the capital of France?\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]` (must be formatted as a valid Python StructuredOutput), and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The question asks for the capital of France, which is a well-known fact. The capital is Paris, and this information is universally accepted with high confidence.\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "{\"answer\": \"Paris\", \"confidence\": 1.0}\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b3b6e6003b84a7",
   "metadata": {
    "cellUniqueIdByVincent": "141dc"
   },
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7781ee0b13cdd45b",
   "metadata": {
    "cellUniqueIdByVincent": "301a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1] Portugal | Portugal (Portuguese: ] ), officially the Portuguese Republic (Portuguese: \"República Portuguesa\" ] ), is a sovereign state located on the Iberian Peninsula in southwestern Europe. It is the westernmost country of mainland Europe, being bordered to the west and south by the Atlantic Ocean and to the north and east by Spain. The Portugal–Spain border is 1214 km long, making it the longest uninterrupted border within the European Union. The republic also includes the Atlantic archipelagos of the Azores and Madeira, both autonomous regions with their own regional governments. \n",
      "\n",
      "2] Portugal (disambiguation) | Portugal is a country in southwestern Europe. \n",
      "\n",
      "3] Geography of Portugal | Portugal is a coastal nation in southwestern Europe, located at the western end of the Iberian Peninsula, bordering Spain (on its northern and eastern frontiers: a total of 1214 km ). The Portuguese territory also includes a series of archipelagos in the Atlantic Ocean (the Açores and Madeira), which are strategic islands along the North Atlantic. The extreme south is not too far from the Strait of Gibraltar, leading to the Mediterranean Sea. In total, the country occupies an area of 92090 km2 of which 91470 km2 is land and 620 km2 water. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "colbertv2_wiki17_abstracts = dspy.ColBERTv2(\n",
    "    url=\"http://20.102.90.50:2017/wiki17_abstracts\"\n",
    ")\n",
    "dspy.settings.configure(rm=colbertv2_wiki17_abstracts)\n",
    "# Define Retrieve Module\n",
    "retriever = dspy.Retrieve(k=3)\n",
    "\n",
    "query = \"Portugal\"\n",
    "\n",
    "# Call the retriever on a particular query.\n",
    "topK_passages = retriever(query).passages\n",
    "\n",
    "for idx, passage in enumerate(topK_passages):\n",
    "    print(f\"{idx + 1}]\", passage, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd75b8a5c00f950b",
   "metadata": {
    "cellUniqueIdByVincent": "471cf"
   },
   "outputs": [],
   "source": [
    "class GenerateAnswer(Signature):\n",
    "    \"\"\"Represents the structure and functionality for generating an answer.\n",
    "\n",
    "    This class is used to define the input fields related to context and\n",
    "    question, and an output field to hold the generated answer. It serves\n",
    "    as a framework for handling the flow of data involved in generating\n",
    "    answers based on an input question and its related context.\n",
    "\n",
    "    Attributes:\n",
    "        context: May contain relevant information.\n",
    "        question: Question to answer.\n",
    "        answer: Answer to the question.\n",
    "    \"\"\"\n",
    "\n",
    "    context = InputField(desc=\"May contain relevant information\")\n",
    "    question = InputField(desc=\"Question to answer\")\n",
    "    answer = OutputField(desc=\"Answer to the question often between 2-7 words\")\n",
    "\n",
    "\n",
    "class RAG(dspy.Module):\n",
    "    \"\"\"\n",
    "    A class representing a Retrieval-Augmented Generation (RAG) model.\n",
    "\n",
    "    This class provides a modular framework for retrieval-augmented generation,\n",
    "    where a retrieval component fetches relevant documents or passages based\n",
    "    on a query, and a generative component produces an output by combining\n",
    "    retrieved information with the query. The purpose of this class is to\n",
    "    combine retrieval and generation capabilities into a cohesive, easy-to-use\n",
    "    pipeline.\n",
    "\n",
    "    Attributes:\n",
    "        num_messages (int): The number of top messages or passages to retrieve,\n",
    "            which indirectly dictates the size of the retrieved context.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_messages=3):\n",
    "        super().__init__()\n",
    "        self.retrieve = dspy.Retrieve(k=num_messages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c832592770667",
   "metadata": {
    "cellUniqueIdByVincent": "b3b39"
   },
   "outputs": [],
   "source": [
    "question = \"What country did Pele play for?\"\n",
    "rag = RAG()\n",
    "output = rag(question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce81ce27103c37d",
   "metadata": {
    "cellUniqueIdByVincent": "8089a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning=\"The question asks about the country Pele played for, but there are multiple footballers named Pelé in the context. The first Pelé (born 1991) plays for the Guinea-Bissau national team. The second mention is about Waldemar de Brito, who discovered Pelé (the famous one, not detailed here). The third Pelé (born 1978) is Cape Verdean. Since the question does not specify which Pelé, and the context provides two distinct national teams (Guinea-Bissau and Cape Verde), it is ambiguous. However, the most famous Pelé (Edson Arantes do Nascimento) is not in this context, so based solely on the provided information, both are valid but the question is unclear. Given the context entries, the answer should reflect the ambiguity or note the possibilities. Since the instruction is to provide an answer between 2-7 words, I'll list the countries mentioned for the Pelés in the context.\",\n",
       "    answer='Guinea-Bissau or Cape Verde.'\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3174d87e27c5cd",
   "metadata": {
    "cellUniqueIdByVincent": "08f9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-09-01T21:35:05.243732]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `context` (str): May contain relevant information\n",
      "2. `question` (str): Question to answer\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): \n",
      "2. `answer` (str): Answer to the question often between 2-7 words\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## context ## ]]\n",
      "{context}\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "{answer}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Represents the structure and functionality for generating an answer.\n",
      "        \n",
      "        This class is used to define the input fields related to context and\n",
      "        question, and an output field to hold the generated answer. It serves\n",
      "        as a framework for handling the flow of data involved in generating\n",
      "        answers based on an input question and its related context.\n",
      "        \n",
      "        Attributes:\n",
      "            context: May contain relevant information.\n",
      "            question: Question to answer.\n",
      "            answer: Answer to the question.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## context ## ]]\n",
      "[1] «Pelé (footballer, born 1991) | Judilson Mamadu Tuncará Gomes (born 29 September 1991), known as Pelé, is a professional footballer who plays as a midfielder for Rio Ave F.C. and the Guinea-Bissau national team.»\n",
      "[2] «Waldemar de Brito | Waldemar de Brito (] , 17 May 1913 – 21 February 1979) was a Brazilian footballer who played as a forward for several clubs in Brazil and Argentina, as well as for the Brazil national team. He is acknowledged to have discovered Pelé during the latter's early footballing days. His brother, Petronilho de Brito, was also a footballer.»\n",
      "[3] «Pelé (footballer, born 1978) | Pedro Miguel Cardoso Monteiro (born 2 May 1978), commonly known as Pelé, is a Cape Verdean footballer who plays as a central defender or a defensive midfielder for English club Totton.»\n",
      "\n",
      "[[ ## question ## ]]\n",
      "What country did Pele play for?\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The question asks about the country Pele played for, but there are multiple footballers named Pelé in the context. The first Pelé (born 1991) plays for the Guinea-Bissau national team. The second mention is about Waldemar de Brito, who discovered Pelé (the famous one, not detailed here). The third Pelé (born 1978) is Cape Verdean. Since the question does not specify which Pelé, and the context provides two distinct national teams (Guinea-Bissau and Cape Verde), it is ambiguous. However, the most famous Pelé (Edson Arantes do Nascimento) is not in this context, so based solely on the provided information, both are valid but the question is unclear. Given the context entries, the answer should reflect the ambiguity or note the possibilities. Since the instruction is to provide an answer between 2-7 words, I'll list the countries mentioned for the Pelés in the context.\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "Guinea-Bissau or Cape Verde.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb99aaf02e95e18",
   "metadata": {
    "cellUniqueIdByVincent": "650cd"
   },
   "source": [
    "# Multihop RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95d3a45f654067",
   "metadata": {
    "cellUniqueIdByVincent": "b82bd"
   },
   "outputs": [],
   "source": [
    "def deduplicate(passages):\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for p in passages:\n",
    "        key = p.strip().lower()\n",
    "        if key not in seen:\n",
    "            unique.append(p)\n",
    "            seen.add(key)\n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d65413185ea0047",
   "metadata": {
    "cellUniqueIdByVincent": "5b74a"
   },
   "outputs": [],
   "source": [
    "class GenerateQuery(Signature):\n",
    "    \"\"\"Represents the structure and functionality for generating a query.\"\"\"\n",
    "\n",
    "    context = InputField(desc=\"May contain relevant information\")\n",
    "    question = InputField(desc=\"Question to answer\")\n",
    "    query = OutputField()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MultihopRAG(dspy.Module):\n",
    "    passages_per_hop: int = 3\n",
    "    max_hops: int = 3\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_query = [\n",
    "            dspy.ChainOfThought(GenerateQuery) for _ in range(self.max_hops)\n",
    "        ]\n",
    "        self.retrieve = dspy.Retrieve(k=self.passages_per_hop)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "        self.rag = RAG(num_messages=self.passages_per_hop)\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = []\n",
    "        for hop in range(self.max_hops):\n",
    "            query = self.generate_query[hop](context=context, question=question).query\n",
    "            passages = self.retrieve(query).passages\n",
    "            context = deduplicate(context + passages)\n",
    "            pred = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(answer=pred.answer, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda205dffd922f5",
   "metadata": {
    "cellUniqueIdByVincent": "7060b"
   },
   "outputs": [],
   "source": [
    "question = \"What is the capital city of the country that won the 2014 world cup?\"\n",
    "multihop_rag = MultihopRAG()\n",
    "output = multihop_rag(question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cd5d6173f43f55",
   "metadata": {
    "cellUniqueIdByVincent": "fc02e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer='Berlin',\n",
       "    context=[\"2014 FIFA World Cup | The 2014 FIFA World Cup was the 20th FIFA World Cup, the quadrennial world championship for men's national football teams organized by FIFA. It took place in Brazil from 12 June to 13 July 2014, after the country was awarded the hosting rights in 2007. It was the second time that Brazil staged the competition, the first being in 1950, and the fifth time that it was held in South America.\", \"2014 FIFA World Cup Final | The 2014 FIFA World Cup Final was a football match that took place on 13 July 2014 at the Maracanã Stadium in Rio de Janeiro, Brazil to determine the 2014 FIFA World Cup champion. Germany defeated Argentina 1–0 in extra time, with the only goal being scored by Mario Götze, who collected André Schürrle's cross from the left on his chest before volleying a high left-footed shot into the net. The match was the third final between the two countries, a World Cup record, after their 1986 and 1990 matches, and billed as the world's best player (Lionel Messi) versus the world's best team (Germany).\", '2014 FIFA World Cup Group D | Group D of the 2014 FIFA World Cup consisted of Uruguay, Costa Rica, England, and Italy. This was the only group to contain more than one previous winner of the World Cup, as there were three previous winners. It was also the only group with three top 10 FIFA World Ranking teams as October 2013 (ranking date for final draw) and at start of competition. Play began on 14 June and ended on 24 June 2014.', 'Capital of Germany | The capital of Germany is the city state of Berlin. It is the seat of the President of Germany, whose official residence is Schloss Bellevue. The Bundesrat (\"federal council\") is the representation of the Federal States (\"Bundesländer\") of Germany and has its seat at the former Prussian Herrenhaus (House of Lords). Though most of the ministries are seated in Berlin, some of them, as well as some minor departments, are seated in Bonn, the former capital of West Germany.', 'Berlin | Berlin ( , ] ) is the capital and the largest city of Germany as well as one of its 16 constituent states. With a population of approximately 3.7 million, Berlin is the second most populous city proper in the European Union and the seventh most populous urban area in the European Union. Located in northeastern Germany on the banks of the rivers Spree and Havel, it is the centre of the Berlin-Brandenburg Metropolitan Region, which has roughly 6 million residents from more than 180 nations.', 'Geography of Berlin | Berlin is the capital city of Germany and one of the 16 states of Germany. With a population of 3.4 million people, Berlin is the second most populous city proper, the seventh most populous urban area in the European Union, and the largest German city.', 'Yes–no question | In linguistics, a yes–no question, formally known as a polar question, is a question whose expected answer is either \"yes\" or \"no\". Formally, they present an exclusive disjunction, a pair of alternatives of which only one is acceptable. In English, such questions can be formed in both positive and negative forms (e.g. \"Will you be here tomorrow?\" and \"Won\\'t you be here tomorrow?\") .', 'A-not-A question | In linguistics, an A-not-A question is a polar question that offers two opposite possibilities for the answer. This disjunctive question is predominantly found in Sinitic and some Altaic languages that offers a choice between an affirmative predicate and its negative counterpart. They are functionally regarded as a type of \"yes/no\" question, since they are very similar to a large extent. \"A-not-A\" questions are often interpreted as having a \\'neutral\\' presupposition or is used in a neutral context. This means that the person asking the A-not-A question does not assume the truth value of the proposition expressed by the question.', 'N/a | n/a or N/A is a common abbreviation in tables and lists for the phrase not applicable, not available, or no answer. It is used to indicate when information in a certain table cell is not provided, either because it does not apply to a particular case in question or because the answer is not available.']\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b13e88a35490fa5",
   "metadata": {
    "cellUniqueIdByVincent": "bf60b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-09-01T21:35:31.255446]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `context` (str): May contain relevant information\n",
      "2. `question` (str): Question to answer\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): \n",
      "2. `answer` (str): Answer to the question often between 2-7 words\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## context ## ]]\n",
      "{context}\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "{answer}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Represents the structure and functionality for generating an answer.\n",
      "        \n",
      "        This class is used to define the input fields related to context and\n",
      "        question, and an output field to hold the generated answer. It serves\n",
      "        as a framework for handling the flow of data involved in generating\n",
      "        answers based on an input question and its related context.\n",
      "        \n",
      "        Attributes:\n",
      "            context: May contain relevant information.\n",
      "            question: Question to answer.\n",
      "            answer: Answer to the question.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## context ## ]]\n",
      "[1] «2014 FIFA World Cup | The 2014 FIFA World Cup was the 20th FIFA World Cup, the quadrennial world championship for men's national football teams organized by FIFA. It took place in Brazil from 12 June to 13 July 2014, after the country was awarded the hosting rights in 2007. It was the second time that Brazil staged the competition, the first being in 1950, and the fifth time that it was held in South America.»\n",
      "[2] «2014 FIFA World Cup Final | The 2014 FIFA World Cup Final was a football match that took place on 13 July 2014 at the Maracanã Stadium in Rio de Janeiro, Brazil to determine the 2014 FIFA World Cup champion. Germany defeated Argentina 1–0 in extra time, with the only goal being scored by Mario Götze, who collected André Schürrle's cross from the left on his chest before volleying a high left-footed shot into the net. The match was the third final between the two countries, a World Cup record, after their 1986 and 1990 matches, and billed as the world's best player (Lionel Messi) versus the world's best team (Germany).»\n",
      "[3] «2014 FIFA World Cup Group D | Group D of the 2014 FIFA World Cup consisted of Uruguay, Costa Rica, England, and Italy. This was the only group to contain more than one previous winner of the World Cup, as there were three previous winners. It was also the only group with three top 10 FIFA World Ranking teams as October 2013 (ranking date for final draw) and at start of competition. Play began on 14 June and ended on 24 June 2014.»\n",
      "[4] «Capital of Germany | The capital of Germany is the city state of Berlin. It is the seat of the President of Germany, whose official residence is Schloss Bellevue. The Bundesrat (\"federal council\") is the representation of the Federal States (\"Bundesländer\") of Germany and has its seat at the former Prussian Herrenhaus (House of Lords). Though most of the ministries are seated in Berlin, some of them, as well as some minor departments, are seated in Bonn, the former capital of West Germany.»\n",
      "[5] «Berlin | Berlin ( , ] ) is the capital and the largest city of Germany as well as one of its 16 constituent states. With a population of approximately 3.7 million, Berlin is the second most populous city proper in the European Union and the seventh most populous urban area in the European Union. Located in northeastern Germany on the banks of the rivers Spree and Havel, it is the centre of the Berlin-Brandenburg Metropolitan Region, which has roughly 6 million residents from more than 180 nations.»\n",
      "[6] «Geography of Berlin | Berlin is the capital city of Germany and one of the 16 states of Germany. With a population of 3.4 million people, Berlin is the second most populous city proper, the seventh most populous urban area in the European Union, and the largest German city.»\n",
      "[7] «Yes–no question | In linguistics, a yes–no question, formally known as a polar question, is a question whose expected answer is either \"yes\" or \"no\". Formally, they present an exclusive disjunction, a pair of alternatives of which only one is acceptable. In English, such questions can be formed in both positive and negative forms (e.g. \"Will you be here tomorrow?\" and \"Won't you be here tomorrow?\") .»\n",
      "[8] «A-not-A question | In linguistics, an A-not-A question is a polar question that offers two opposite possibilities for the answer. This disjunctive question is predominantly found in Sinitic and some Altaic languages that offers a choice between an affirmative predicate and its negative counterpart. They are functionally regarded as a type of \"yes/no\" question, since they are very similar to a large extent. \"A-not-A\" questions are often interpreted as having a 'neutral' presupposition or is used in a neutral context. This means that the person asking the A-not-A question does not assume the truth value of the proposition expressed by the question.»\n",
      "[9] «N/a | n/a or N/A is a common abbreviation in tables and lists for the phrase not applicable, not available, or no answer. It is used to indicate when information in a certain table cell is not provided, either because it does not apply to a particular case in question or because the answer is not available.»\n",
      "\n",
      "[[ ## question ## ]]\n",
      "What is the capital city of the country that won the 2014 world cup?\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The question asks for the capital city of the country that won the 2014 World Cup. From context [2], it is stated that Germany defeated Argentina in the final, so Germany is the winner. From context [4] and [5], the capital of Germany is Berlin. Therefore, the answer is Berlin.\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "Berlin\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981e17aae4d92a97",
   "metadata": {
    "cellUniqueIdByVincent": "05f61"
   },
   "source": [
    "# Zero Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a8b0d6b47dbc8d",
   "metadata": {
    "cellUniqueIdByVincent": "1ee7d"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"football_rules_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7f7e2b6d57ed0e",
   "metadata": {
    "cellUniqueIdByVincent": "c8a4f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Player spits on an opponent</td>\n",
       "      <td>Red card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Player deliberately handles the ball to preven...</td>\n",
       "      <td>Red card and penalty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Player commits a reckless tackle</td>\n",
       "      <td>Yellow card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Player shows dissent by word or action</td>\n",
       "      <td>Yellow card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Player delays the restart of play</td>\n",
       "      <td>Yellow card</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question                Answer\n",
       "0                        Player spits on an opponent              Red card\n",
       "1  Player deliberately handles the ball to preven...  Red card and penalty\n",
       "2                   Player commits a reckless tackle           Yellow card\n",
       "3             Player shows dissent by word or action           Yellow card\n",
       "4                  Player delays the restart of play           Yellow card"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4277addafa0f4852",
   "metadata": {
    "cellUniqueIdByVincent": "06cb5"
   },
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "trainset, testset = train_test_split(df, test_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588fe79aca67607c",
   "metadata": {
    "cellUniqueIdByVincent": "35851"
   },
   "outputs": [],
   "source": [
    "train_set = [\n",
    "    Example(question=x[\"Question\"], answer=x[\"Answer\"]).with_inputs(\"question\")\n",
    "    for x in trainset.to_dict(orient=\"records\")\n",
    "]\n",
    "test_set = [\n",
    "    Example(question=x[\"Question\"], answer=x[\"Answer\"]).with_inputs(\"question\")\n",
    "    for x in testset.to_dict(orient=\"records\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35198daf820961d6",
   "metadata": {
    "cellUniqueIdByVincent": "925ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Example({'question': \"Player commits flagrant elbow to opponent's face\", 'answer': 'Red card and criminal charges'}) (input_keys={'question'}),\n",
       " Example({'question': 'Player throws punch at opponent after final whistle', 'answer': 'Red card and criminal charges'}) (input_keys={'question'}),\n",
       " Example({'question': 'Player commits violent conduct off the ball', 'answer': 'Red card'}) (input_keys={'question'}),\n",
       " Example({'question': 'Player makes borderline tackle timing', 'answer': 'Warning'}) (input_keys={'question'}),\n",
       " Example({'question': 'Player impedes an opponent without playing the ball', 'answer': 'Indirect free kick'}) (input_keys={'question'}),\n",
       " Example({'question': 'Player commits soft contact during aerial duel', 'answer': 'Play on'}) (input_keys={'question'}),\n",
       " Example({'question': 'Player commits minor obstruction of opponent', 'answer': 'Warning'}) (input_keys={'question'}),\n",
       " Example({'question': 'Player commits light holding during corner', 'answer': 'Warning'}) (input_keys={'question'}),\n",
       " Example({'question': 'Player appeals for penalty without aggression', 'answer': 'Play on'}) (input_keys={'question'}),\n",
       " Example({'question': 'Player makes death threats to opponent', 'answer': 'Red card and criminal charges'}) (input_keys={'question'}),\n",
       " Example({'question': 'Player commits light pushing during set piece', 'answer': 'Warning'}) (input_keys={'question'}),\n",
       " Example({'question': 'Player deliberately handles the ball to prevent a goal', 'answer': 'Red card and penalty'}) (input_keys={'question'}),\n",
       " Example({'question': 'Player shows mild dissent through body language', 'answer': 'Warning'}) (input_keys={'question'}),\n",
       " Example({'question': 'Player throws elbow with intent to injure', 'answer': 'Red card and lengthy ban'}) (input_keys={'question'}),\n",
       " Example({'question': 'Player commits minor delay of game', 'answer': 'Warning'}) (input_keys={'question'}),\n",
       " Example({'question': \"Player commits flying kick to opponent's chest\", 'answer': 'Red card and criminal charges'}) (input_keys={'question'}),\n",
       " Example({'question': 'Player spits at match official', 'answer': 'Red card and criminal charges'}) (input_keys={'question'}),\n",
       " Example({'question': 'Player throws haymaker punch during mass confrontation', 'answer': 'Red card and criminal charges'}) (input_keys={'question'}),\n",
       " Example({'question': 'Player commits charged shoulder-to-shoulder fairly', 'answer': 'Play on'}) (input_keys={'question'}),\n",
       " Example({'question': \"Player commits reckless stamp on opponent's hand\", 'answer': 'Red card and lengthy ban'}) (input_keys={'question'})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6a4c75f831775d",
   "metadata": {
    "cellUniqueIdByVincent": "cd93f"
   },
   "outputs": [],
   "source": [
    "RefereeOptions = Literal[\n",
    "    \"Red card and criminal charges\",\n",
    "    \"Yellow card\",\n",
    "    \"Red card\",\n",
    "    \"Warning\",\n",
    "    \"Warning and retake\",\n",
    "    \"Warning or yellow card\",\n",
    "    \"Play on\",\n",
    "    \"Red card and lengthy ban\",\n",
    "    \"Indirect free kick\",\n",
    "    \"Free kick\",\n",
    "    \"Penalty and red card\",\n",
    "    \"Yellow card or penalty if in box\",\n",
    "    \"Red card and penalty\",\n",
    "    \"Red card and penalty if in box\",\n",
    "]\n",
    "\n",
    "\n",
    "class RefereeAnswer(Signature):\n",
    "    \"\"\"Represents a referee's answer for a specific question.\n",
    "\n",
    "    This class is derived from the `Signature` class and is used to\n",
    "    provide functionality for managing a referee's response to a question.\n",
    "    It defines the input question and the expected output answer, which\n",
    "    should involve selecting an appropriate action strictly without any\n",
    "    paraphrasing.\n",
    "\n",
    "    Attributes:\n",
    "        question (InputField): The input question that requires the referee's\n",
    "            decision or action.\n",
    "        answer (OutputField): The referee's selected action, defined by the\n",
    "            `RefereeOptions` type, ensuring it is chosen without paraphrasing.\n",
    "    \"\"\"\n",
    "\n",
    "    question = InputField()\n",
    "    answer = OutputField(\n",
    "        desc=\"MANDATORY: Select one exact option from this list without any modification: ['Red card and criminal charges', 'Yellow card', 'Red card', 'Warning', 'Warning and retake', 'Warning or yellow card', 'Play on', 'Red card and lengthy ban', 'Indirect free kick', 'Free kick', 'Penalty and red card', 'Yellow card or penalty if in box', 'Red card and penalty', 'Red card and penalty if in box']\",\n",
    "        type=RefereeOptions,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b5ce858b698c37",
   "metadata": {
    "cellUniqueIdByVincent": "c9875"
   },
   "outputs": [],
   "source": [
    "class PredictModel(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.predict = ChainOfThought(RefereeAnswer)\n",
    "\n",
    "    def forward(self, question):\n",
    "        prediction = self.predict(question=question)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9389f004560133ab",
   "metadata": {
    "cellUniqueIdByVincent": "341df"
   },
   "outputs": [],
   "source": [
    "predict = PredictModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56562bf2590c20c",
   "metadata": {
    "cellUniqueIdByVincent": "fb520"
   },
   "outputs": [],
   "source": [
    "evaluate_program = Evaluate(\n",
    "    devset=test_set,\n",
    "    metric=answer_exact_match,  # More flexible than exact match\n",
    "    display_table=10,\n",
    "    display_progress=True,\n",
    "    num_threads=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bae5ad82207eb8",
   "metadata": {
    "cellUniqueIdByVincent": "dc9f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 46.00 / 83 (55.4%): 100%|██████████| 83/83 [00:03<00:00, 24.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/01 21:35:36 INFO dspy.evaluate.evaluate: Average Metric: 46 / 83 (55.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>example_answer</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>pred_answer</th>\n",
       "      <th>answer_exact_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Player commits sexual assault during play</td>\n",
       "      <td>Red card and criminal charges</td>\n",
       "      <td>Sexual assault is a severe criminal offense that goes far beyond t...</td>\n",
       "      <td>Red card and criminal charges</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Player makes questionable appeal for foul</td>\n",
       "      <td>Play on</td>\n",
       "      <td>The scenario describes a player making a \"questionable appeal for ...</td>\n",
       "      <td>Yellow card</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Player throws bottle at referee from bench</td>\n",
       "      <td>Red card and criminal charges</td>\n",
       "      <td>Throwing an object, especially a bottle, at the referee from the b...</td>\n",
       "      <td>Red card and criminal charges</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Player blocks goalkeeper's release of the ball</td>\n",
       "      <td>Yellow card</td>\n",
       "      <td>Blocking the goalkeeper's release of the ball is considered an off...</td>\n",
       "      <td>Yellow card</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Player pulls opponent's hair during play</td>\n",
       "      <td>Yellow card</td>\n",
       "      <td>Pulling an opponent's hair is a serious offense that is considered...</td>\n",
       "      <td>Red card</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Player simulates contact to deceive referee</td>\n",
       "      <td>Yellow card</td>\n",
       "      <td>Simulating contact to deceive the referee is an act of unsporting ...</td>\n",
       "      <td>Yellow card</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Player pushes an opponent during corner kick</td>\n",
       "      <td>Warning or yellow card</td>\n",
       "      <td>Pushing an opponent is a direct physical foul that is unsporting a...</td>\n",
       "      <td>Yellow card</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Player headbutts an opponent</td>\n",
       "      <td>Red card and criminal charges</td>\n",
       "      <td>A headbutt is a violent and aggressive act that is considered seri...</td>\n",
       "      <td>Red card</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Player commits a late tackle after ball has gone</td>\n",
       "      <td>Yellow card</td>\n",
       "      <td>A late tackle, especially after the ball has gone, is considered r...</td>\n",
       "      <td>Yellow card</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Player enters opponent's penalty area before penalty kick</td>\n",
       "      <td>Warning and retake</td>\n",
       "      <td>According to the Laws of the Game (Law 14), if an attacking player...</td>\n",
       "      <td>Warning and retake</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    question  \\\n",
       "0                  Player commits sexual assault during play   \n",
       "1                  Player makes questionable appeal for foul   \n",
       "2                 Player throws bottle at referee from bench   \n",
       "3             Player blocks goalkeeper's release of the ball   \n",
       "4                   Player pulls opponent's hair during play   \n",
       "5                Player simulates contact to deceive referee   \n",
       "6               Player pushes an opponent during corner kick   \n",
       "7                               Player headbutts an opponent   \n",
       "8           Player commits a late tackle after ball has gone   \n",
       "9  Player enters opponent's penalty area before penalty kick   \n",
       "\n",
       "                  example_answer  \\\n",
       "0  Red card and criminal charges   \n",
       "1                        Play on   \n",
       "2  Red card and criminal charges   \n",
       "3                    Yellow card   \n",
       "4                    Yellow card   \n",
       "5                    Yellow card   \n",
       "6         Warning or yellow card   \n",
       "7  Red card and criminal charges   \n",
       "8                    Yellow card   \n",
       "9             Warning and retake   \n",
       "\n",
       "                                                               reasoning  \\\n",
       "0  Sexual assault is a severe criminal offense that goes far beyond t...   \n",
       "1  The scenario describes a player making a \"questionable appeal for ...   \n",
       "2  Throwing an object, especially a bottle, at the referee from the b...   \n",
       "3  Blocking the goalkeeper's release of the ball is considered an off...   \n",
       "4  Pulling an opponent's hair is a serious offense that is considered...   \n",
       "5  Simulating contact to deceive the referee is an act of unsporting ...   \n",
       "6  Pushing an opponent is a direct physical foul that is unsporting a...   \n",
       "7  A headbutt is a violent and aggressive act that is considered seri...   \n",
       "8  A late tackle, especially after the ball has gone, is considered r...   \n",
       "9  According to the Laws of the Game (Law 14), if an attacking player...   \n",
       "\n",
       "                     pred_answer answer_exact_match  \n",
       "0  Red card and criminal charges          ✔️ [True]  \n",
       "1                    Yellow card                     \n",
       "2  Red card and criminal charges          ✔️ [True]  \n",
       "3                    Yellow card          ✔️ [True]  \n",
       "4                       Red card                     \n",
       "5                    Yellow card          ✔️ [True]  \n",
       "6                    Yellow card                     \n",
       "7                       Red card                     \n",
       "8                    Yellow card          ✔️ [True]  \n",
       "9             Warning and retake          ✔️ [True]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div style='\n",
       "                text-align: center;\n",
       "                font-size: 16px;\n",
       "                font-weight: bold;\n",
       "                color: #555;\n",
       "                margin: 10px 0;'>\n",
       "                ... 73 more rows not displayed ...\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(score=55.42, results=<list of 83 results>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval = evaluate_program(predict)\n",
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925009d0c4238650",
   "metadata": {
    "cellUniqueIdByVincent": "d8039"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"question\": ex.question,\n",
    "            \"expected_answer\": ex.answer,\n",
    "            \"predicted_answer\": pred.answer,\n",
    "            \"reasoning\": pred.reasoning,\n",
    "            \"correct\": correct,\n",
    "        }\n",
    "        for ex, pred, correct in eval.results\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cdfd14234db004",
   "metadata": {
    "cellUniqueIdByVincent": "8c476"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Player engages in time-wasting tactics</td>\n",
       "      <td>Yellow card</td>\n",
       "      <td>Yellow card</td>\n",
       "      <td>Time-wasting tactics are considered unsporting behavior under the ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Player makes obscene gesture to family section</td>\n",
       "      <td>Red card and criminal charges</td>\n",
       "      <td>Red card</td>\n",
       "      <td>Making an obscene gesture, especially directed toward a family sec...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Player makes contact with referee accidentally</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Play on</td>\n",
       "      <td>The Laws of the Game (IFAB) state that physical contact with a mat...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Player removes shirt during goal celebration</td>\n",
       "      <td>Yellow card</td>\n",
       "      <td>Yellow card</td>\n",
       "      <td>According to the Laws of the Game (Law 12), removing the shirt dur...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Player slightly delays throw-in restart</td>\n",
       "      <td>Play on</td>\n",
       "      <td>Warning</td>\n",
       "      <td>A slight delay in taking a throw-in is considered unsporting behav...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question  \\\n",
       "78          Player engages in time-wasting tactics   \n",
       "79  Player makes obscene gesture to family section   \n",
       "80  Player makes contact with referee accidentally   \n",
       "81    Player removes shirt during goal celebration   \n",
       "82         Player slightly delays throw-in restart   \n",
       "\n",
       "                  expected_answer predicted_answer  \\\n",
       "78                    Yellow card      Yellow card   \n",
       "79  Red card and criminal charges         Red card   \n",
       "80                        Warning          Play on   \n",
       "81                    Yellow card      Yellow card   \n",
       "82                        Play on          Warning   \n",
       "\n",
       "                                                                reasoning  \\\n",
       "78  Time-wasting tactics are considered unsporting behavior under the ...   \n",
       "79  Making an obscene gesture, especially directed toward a family sec...   \n",
       "80  The Laws of the Game (IFAB) state that physical contact with a mat...   \n",
       "81  According to the Laws of the Game (Law 12), removing the shirt dur...   \n",
       "82  A slight delay in taking a throw-in is considered unsporting behav...   \n",
       "\n",
       "    correct  \n",
       "78     True  \n",
       "79    False  \n",
       "80    False  \n",
       "81     True  \n",
       "82    False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38a803f0c1b06df",
   "metadata": {
    "cellUniqueIdByVincent": "5d800"
   },
   "source": [
    "# Few shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c35fd7e72d1458",
   "metadata": {
    "cellUniqueIdByVincent": "6e7cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:02<00:02,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 11 examples for up to 1 rounds, amounting to 11 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "teleprompter = BootstrapFewShot(metric=answer_exact_match, max_labeled_demos=10)\n",
    "compiled_predictor = teleprompter.compile(predict, trainset=train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fe76053037b11e",
   "metadata": {
    "cellUniqueIdByVincent": "30e0a"
   },
   "outputs": [],
   "source": [
    "evaluate_program = Evaluate(\n",
    "    devset=test_set,\n",
    "    metric=answer_exact_match,\n",
    "    display_table=10,\n",
    "    num_threads=8,\n",
    "    display_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe32a60da36daaa",
   "metadata": {
    "cellUniqueIdByVincent": "83c66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 50.00 / 83 (60.2%): 100%|██████████| 83/83 [00:04<00:00, 18.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/01 21:35:45 INFO dspy.evaluate.evaluate: Average Metric: 50 / 83 (60.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>example_answer</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>pred_answer</th>\n",
       "      <th>answer_exact_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Player commits sexual assault during play</td>\n",
       "      <td>Red card and criminal charges</td>\n",
       "      <td>Sexual assault is an extremely serious criminal act that has no pl...</td>\n",
       "      <td>Red card and criminal charges</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Player makes questionable appeal for foul</td>\n",
       "      <td>Play on</td>\n",
       "      <td>Questionable appeals for a foul are considered acts of unsporting ...</td>\n",
       "      <td>Warning</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Player throws bottle at referee from bench</td>\n",
       "      <td>Red card and criminal charges</td>\n",
       "      <td>Throwing a bottle at the referee is an act of violent conduct and ...</td>\n",
       "      <td>Red card and criminal charges</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Player blocks goalkeeper's release of the ball</td>\n",
       "      <td>Yellow card</td>\n",
       "      <td>Blocking the goalkeeper's release of the ball is an offense that p...</td>\n",
       "      <td>Indirect free kick</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Player pulls opponent's hair during play</td>\n",
       "      <td>Yellow card</td>\n",
       "      <td>Pulling an opponent's hair is an act of violent conduct, as it is ...</td>\n",
       "      <td>Red card</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Player simulates contact to deceive referee</td>\n",
       "      <td>Yellow card</td>\n",
       "      <td>Simulating contact (diving) to deceive the referee is an act of un...</td>\n",
       "      <td>Yellow card</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Player pushes an opponent during corner kick</td>\n",
       "      <td>Warning or yellow card</td>\n",
       "      <td>Pushing an opponent during a corner kick is a foul, as it is a for...</td>\n",
       "      <td>Warning</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Player headbutts an opponent</td>\n",
       "      <td>Red card and criminal charges</td>\n",
       "      <td>A headbutt is an act of violent conduct, which is a serious foul p...</td>\n",
       "      <td>Red card and criminal charges</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Player commits a late tackle after ball has gone</td>\n",
       "      <td>Yellow card</td>\n",
       "      <td>A late tackle after the ball has gone is a reckless challenge that...</td>\n",
       "      <td>Red card</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Player enters opponent's penalty area before penalty kick</td>\n",
       "      <td>Warning and retake</td>\n",
       "      <td>Entering the opponent's penalty area before a penalty kick is take...</td>\n",
       "      <td>Warning and retake</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    question  \\\n",
       "0                  Player commits sexual assault during play   \n",
       "1                  Player makes questionable appeal for foul   \n",
       "2                 Player throws bottle at referee from bench   \n",
       "3             Player blocks goalkeeper's release of the ball   \n",
       "4                   Player pulls opponent's hair during play   \n",
       "5                Player simulates contact to deceive referee   \n",
       "6               Player pushes an opponent during corner kick   \n",
       "7                               Player headbutts an opponent   \n",
       "8           Player commits a late tackle after ball has gone   \n",
       "9  Player enters opponent's penalty area before penalty kick   \n",
       "\n",
       "                  example_answer  \\\n",
       "0  Red card and criminal charges   \n",
       "1                        Play on   \n",
       "2  Red card and criminal charges   \n",
       "3                    Yellow card   \n",
       "4                    Yellow card   \n",
       "5                    Yellow card   \n",
       "6         Warning or yellow card   \n",
       "7  Red card and criminal charges   \n",
       "8                    Yellow card   \n",
       "9             Warning and retake   \n",
       "\n",
       "                                                               reasoning  \\\n",
       "0  Sexual assault is an extremely serious criminal act that has no pl...   \n",
       "1  Questionable appeals for a foul are considered acts of unsporting ...   \n",
       "2  Throwing a bottle at the referee is an act of violent conduct and ...   \n",
       "3  Blocking the goalkeeper's release of the ball is an offense that p...   \n",
       "4  Pulling an opponent's hair is an act of violent conduct, as it is ...   \n",
       "5  Simulating contact (diving) to deceive the referee is an act of un...   \n",
       "6  Pushing an opponent during a corner kick is a foul, as it is a for...   \n",
       "7  A headbutt is an act of violent conduct, which is a serious foul p...   \n",
       "8  A late tackle after the ball has gone is a reckless challenge that...   \n",
       "9  Entering the opponent's penalty area before a penalty kick is take...   \n",
       "\n",
       "                     pred_answer answer_exact_match  \n",
       "0  Red card and criminal charges          ✔️ [True]  \n",
       "1                        Warning                     \n",
       "2  Red card and criminal charges          ✔️ [True]  \n",
       "3             Indirect free kick                     \n",
       "4                       Red card                     \n",
       "5                    Yellow card          ✔️ [True]  \n",
       "6                        Warning                     \n",
       "7  Red card and criminal charges          ✔️ [True]  \n",
       "8                       Red card                     \n",
       "9             Warning and retake          ✔️ [True]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div style='\n",
       "                text-align: center;\n",
       "                font-size: 16px;\n",
       "                font-weight: bold;\n",
       "                color: #555;\n",
       "                margin: 10px 0;'>\n",
       "                ... 73 more rows not displayed ...\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_compiled = evaluate_program(compiled_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439ea4b1b8766c1",
   "metadata": {
    "cellUniqueIdByVincent": "8d15e"
   },
   "source": [
    "# Refine x BestofN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344e5b49b57cf499",
   "metadata": {
    "cellUniqueIdByVincent": "d6ef1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer='Warning or yellow card'\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PredictModel(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.predict = ChainOfThought(RefereeAnswer)\n",
    "\n",
    "    def forward(self, question):\n",
    "        output = self.predict(question=question)\n",
    "        return dspy.Prediction(answer=output.answer)\n",
    "\n",
    "\n",
    "predict_model = PredictModel()\n",
    "predict_model(\n",
    "    question=\"Player makes questionable appeal during foul\"\n",
    ")  # Shoulda been yellow card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25be1d41b761695",
   "metadata": {
    "cellUniqueIdByVincent": "a46e0"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "By default, Refine will try to run the module up to N times until the threshold is met. If the module encounters an error, it will keep going up to N failed attempts\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Reward function: check if answer is valid\n",
    "def reward_fn(example, prediction, trace=None):\n",
    "    allowed_answers = [\n",
    "        \"Red card and criminal charges\",\n",
    "        \"Yellow card\",\n",
    "        \"Red card\",\n",
    "        \"Warning\",\n",
    "        \"Warning and retake\",\n",
    "        \"Warning or yellow card\",\n",
    "        \"Play on\",\n",
    "        \"Red card and lengthy ban\",\n",
    "        \"Indirect free kick\",\n",
    "        \"Free kick\",\n",
    "        \"Penalty and red card\",\n",
    "        \"Yellow card or penalty if in box\",\n",
    "        \"Red card and penalty\",\n",
    "        \"Red card and penalty if in box\",\n",
    "    ]\n",
    "    return 1.0 if prediction.answer in allowed_answers else 0.0\n",
    "\n",
    "\n",
    "class PredictModelRefine(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Refine wraps the predictor\n",
    "        self.predict = dspy.Refine(\n",
    "            module=ChainOfThought(RefereeAnswer),\n",
    "            N=5,  # generate 5 candidates\n",
    "            reward_fn=reward_fn,  # use custom reward check\n",
    "            threshold=0.5,  # accept if score ≥ 0.5\n",
    "        )\n",
    "\n",
    "    def forward(self, question):\n",
    "        output = self.predict(question=question)\n",
    "        return Prediction(answer=output.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db9dd75daf8b088",
   "metadata": {
    "cellUniqueIdByVincent": "f09e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer='Yellow card'\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_model = PredictModelRefine()\n",
    "predict_model(\n",
    "    question=\"Player makes questionable appeal during foul\"\n",
    ")  # Outputs the right thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584900882a5f517f",
   "metadata": {
    "cellUniqueIdByVincent": "417f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer='Yellow card'\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Runs a module up to N times with different rollout IDs (bypassing cache) and returns the best prediction, as defined by the reward_fn, or the first prediction that passes the threshold.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class PredictModelBestofN(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Refine wraps the predictor\n",
    "        self.predict = dspy.BestOfN(\n",
    "            module=ChainOfThought(RefereeAnswer),\n",
    "            N=5,  # generate 5 candidates\n",
    "            reward_fn=reward_fn,  # use custom reward check\n",
    "            threshold=0.5,  # accept if score ≥ 0.5\n",
    "        )\n",
    "\n",
    "    def forward(self, question):\n",
    "        output = self.predict(question=question)\n",
    "        return Prediction(answer=output.answer)\n",
    "\n",
    "\n",
    "predict_model = PredictModelBestofN()\n",
    "predict_model(\n",
    "    question=\"Player makes questionable appeal during foul\"\n",
    ")  # Outputs the right thing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Prompt_EngineeringxProgramming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vincent": {
   "sessionId": "358be50fc3650cf42637f14b_2025-09-01T17-24-39-398Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
